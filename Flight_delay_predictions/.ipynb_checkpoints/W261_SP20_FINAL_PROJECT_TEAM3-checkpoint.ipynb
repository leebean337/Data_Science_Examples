{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Final Project - Predicting Airline Delays\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2020`__\n",
    "\n",
    "Team 3:\n",
    "* Tonya Di Sera\n",
    "* Ammara Essa\n",
    "* Andy Hoopengardner\n",
    "* Lee Moore\n",
    "\n",
    "[Presentation slides](https://docs.google.com/presentation/d/1pxUoSHfAK64we-ReFgVAXdrOS0Q8xGsTSj_c7_Ryoq4/edit#slide=id.g8347663c4a_0_486)\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/title.png?raw=true\" width=\"60%\" >\n",
    "\n",
    "# Table of Contents\n",
    "### 1. Question Formulation\n",
    "### 2. EDA and Discussion of Challenges\n",
    "### 3. Feature Engineering\n",
    "### 4. Algorithm Exploration\n",
    "### 5. Algorithm Implementation\n",
    "### 6. Conclusions\n",
    "### 7. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Question Formulation <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/question_formation.png?raw=true\" width=\"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.1 Study objective and dataset\n",
    "\n",
    "Our study focused on the objective of predicting detrimental flight outcomes, specifically flight delays, cancellations or diverted flights, utilising the airline dataset combined with station and weather data. This airline dataset includes 31,746,841 flights from 2015-2019 and joins in available data on location and time-specific weather. We have defined our binary classification task as no delay (=0) versus delay, diverted or cancelled flights (=1), where we applied the national standard definition of delay which is greater than 15 minutes delay to the arrival of the flight. This kind of analysis could prove useful to both travellers and airline companies as an early detection system that customer travel plans may be disrupted, so that alternative plans can be put into place sooner. We aim to build a model that was widely applicable as possible instead of narrowly focusing on specific airports, airlines or specific flight outcomes (i.e. ignoring cancelled or diverted flights). This represents an unbalanced classification task given the rarity of the \"positive\" outcome (defined as delayed, diverted or cancelled in this instance, and subsequently referred to just as \"delayed flights\") which represents roughly 20% of the flights in our dataset. \n",
    "\n",
    "###1.2 Testing approach and Evaluation Metrics\n",
    "\n",
    "Given the time series nature of the study, we chose to reserve the older data from 2015-2018 for our training set and to utilise the most recent data from 2019 for dev and test sets. We randomly sampled 50% of the 2019 data for our dev set and the remaining 50% formed our test set which we have reserved for only the final fine-tuned model predictions. A balanced sampling approach was utilised so we maintained the sampe proportions of our minority/majority class labaels in each datset. We considered it important that our unseen test data should not be sampled from the same dates that our training data comes from, given that the evidence that a model is practically useful depends on its ability to predict delays on future flights.  \n",
    "\n",
    "A review of the literature on airport delay prediction studies suggest that accuracy is the most frequently reported metric but we believe that this is not sufficient since it puts greater weight on correct predictions within the majority class (in this case \"no delays\" which represents 80% of the data) (Etani 2019, Patgiri 2020). We believe that recall is the key metric in this analysis given the opportunity to action on correct predictions regarding delayed flights. However, since precision is also important, given the risk of unnecessary preemptive investment into alternative travel arrangements would cause great frustration to customers, we would argue that the composite measure of F1-score should be our secondary target evaluation metric for optimization, following our primary target of recall optimization. To be practically useful, we believe (based on our collective subjective view and a review of the literature) that recall should be at least .8, meaning that at least 80% of all delayed flights are correctly predicted and precision should be at least .8, meaning that no more than 20% of predicted delays are actually on time. This would mean we are targeting an F1-score of 0.8.\n",
    "\n",
    "###1.3 Baseline and Modelling Approach\n",
    "\n",
    "For our baseline, we selected the naive classifier based on a simple logistic regression model which produces the crude prediction of \"No Delay\" for every flight. Given the unbalanced classification task, this still represents a reasonably strong accuracy of 80% and therefore underlines again the reason that accuracy cannot be the sole metric of interest. This baseline produces a zero value for precision and for recall.  For our algorithms of interest, we will explore optimizations of logistic regression as well as various formulations of decision trees.\n",
    "\n",
    "###1.4 Study Limitations\n",
    "\n",
    "Intuitively, weather will be a critical component to predicting likelihood of a flight delay, however real weather patterns at the time of flight are not available ahead of the flight time when the flight delay predictions would be most useful, e.g. at least several hours ahead of the flight departure. This highlights a disconnect between the data we have available and the reality of this use case. For a machine learning model to be practically useful, it would need to rely on weather _forecasts_ rather than on realized weather patterns. How different weather forecasts are from realized weather patterns will inform how well our training data will support accurate flight delay prediction, though this is not something that can be evaluated through our unseen test data (which again utilises actual realised weather).\n",
    "\n",
    "Additionally, in the real world, a system for predicting delays would rely on a real-time system taking into account delays at connected airports where the relevant aircraft may be transiting through. This real-time system may also take into account evolving weather patterns over the flight paths of the relevant aircraft. These would involve utilising dynamic graph algorithms since the flight paths planned on a specific day may different significantly from the flight path planned the prior week or indeed the prior year. Therefore our traditional approach to training set / test set evaluation would not work in this situation. \n",
    "\n",
    "Another limitation is that for our base model we selected the simple binary task of predicting delay or no delay. However, a more meaningful outcome may require more granular classes such as 'no delay', 'modest delay', 'severe delay' given that different plans may be required in the case of a 1 hour delay compared to a 3 hours delay and additionally knowing the probability of having cancelled or diverted flight outcome would be important for customers. We believe that a two stage approach similar to that identified in the literature (Thiagarajan 2017), where we first predict delay or no delay (as we are doing in this analysis), and then for delayed flights we further predict duration of delay would be appropriate. Due to time constraints we focused just on the first stage of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. EDA and Discussion of Challenges\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/eda.png?raw=true\" width=\"60%\" >\n",
    "\n",
    "Our exploratory data analysis began with building an understanding of each of the variables, their definitions and their summary statistics (including any missing data). Correlation matrices were created to understand bivariate relationships between continuous variables and various histograms, bar charts, and box plots were created to see how various potential features changed over time and over outcome status (delay or not delayed).\n",
    "\n",
    "For additional work on EDA [see separate notebook here](https://dbc-b1c912e7-d804.cloud.databricks.com/?o=7564214546094626#notebook/1239491467269575/command/814519033145399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf, col, lit, when\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets to perform EDA\n",
    "airlines = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/201*.parquet\")\n",
    "airlines.createOrReplaceTempView(\"airlines\")\n",
    "\n",
    "airlines_and_weather = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/user/tonydisera@ischool.berkeley.edu/final_project/data_new_weather/*\")\n",
    "airlines_and_weather.createOrReplaceTempView(\"airlines_and_weather\")\n",
    "airlines_and_weather = spark.sql(\"select YEAR(FL_DATE) as YEAR, * from airlines_and_weather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 EDA: How balanced is our outcome of interest?\n",
    "\n",
    "We decided to focus on arrival delay rather than depature delay due to the more significant impact that this can have on customers. In many instances, a delayed departure can be counteracted by \"making up time\" in the air, in which case the travellers still reach their destination on time. Departure delay was therefore considered less important than an arrival delay. Our EDA showed a classic skewed curve for total minues of delay. For our classification task, we focused on delays of 15 minutes or more and also included cancelled and diverted flights in the definition of our positive outcome. We considered it important to consider all three types of detrimental flight outcomes in our model because they all occur in the real world, and consequently our model trained on this data should be able to correctly identify all of these real life events. \n",
    "\n",
    "Given that the positive outcome is underepresented, this has potential significance for our algorithms of interest. Specifically, due to this finding, we will consider methods to weight or rebalance the training set prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram of arrival delays in minutes\n",
    "plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "bins, counts = airlines.filter(\"ARR_DELAY_NEW  IS NOT NULL\") \\\n",
    "                            .filter(\"ARR_DELAY_NEW  < 120\") \\\n",
    "                            .filter(\"ARR_DELAY_NEW  > 0\") \\\n",
    "                            .selectExpr(\"ARR_DELAY_NEW\").rdd.flatMap(lambda x: x).histogram(30)\n",
    "plt.hist(bins[:-1], bins=bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ARRIVAL_OUTCOME</th><th>count</th></tr></thead><tbody><tr><td>on time</td><td>25586218</td></tr><tr><td>delayed</td><td>6160623</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA on binary outcome variable\n",
    "airlines = airlines.withColumn('ARRIVAL_OUTCOME', f.when((f.col(\"ARR_DELAY_NEW\") > 15) | \\\n",
    "                                          (f.col(\"CANCELLED\") == \"true\") | \\\n",
    "                                          (f.col(\"DIVERTED\") == \"true\"), 'delayed')\\\n",
    "                                          .otherwise('on time'))\n",
    "airlines.createOrReplaceTempView(\"airlines\")\n",
    "display(spark.sql(\"select ARRIVAL_OUTCOME, count(*) as count from airlines group by ARRIVAL_OUTCOME order by count desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 EDA: Is it justified to create a composite outcome of flight delay, cancelled and diverted flights?\n",
    "\n",
    "We justified our composite outcome measure by examining various features with respect to each outcome separately - specifically flight delays greater than 15 minutes, cancelled flights and diverted flights. Our belief was that the attributes which caused cancelled or diverted flights are primarily extreme versions of the reasons that cause flight delays (i.e. heavy rain versus hurricanes). Our EDA on this topic did align with our hypothesis. Specifically, where a weather feature tended to be higher/lower for delayed flights, it tended to be even higher/lower for cancelled and diverted flights. By combining these three outcomes, we were also able to increase the total representation of our 'positive' class. In the original dataset, there are 6,160,623 delayed, cancelled or diverted flights representing 19.4% of all successful flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on cancelled and diverted flight outcomes - to justify composite outcome approach\n",
    "\n",
    "airlines_and_weather = airlines_and_weather.withColumn('OUTCOME', f.when((f.col(\"ARR_DELAY_NEW\") > 15) | \\\n",
    "                                          (f.col(\"CANCELLED\") == \"true\") | \\\n",
    "                                          (f.col(\"DIVERTED\") == \"true\"), 1)\\\n",
    "                                          .otherwise(0))\n",
    "airlines_and_weather.createOrReplaceTempView(\"airlines_and_weather\")\n",
    "df1 = spark.sql(\"select OUTCOME, ORIGIN_CIG_AVG, DEST_SLP_AVG from airlines_and_weather where CANCELLED = 'false' and DIVERTED = 'false'  \").toPandas()\n",
    "df2 = spark.sql(\"select OUTCOME, ORIGIN_CIG_AVG, DEST_SLP_AVG from airlines_and_weather where CANCELLED = 'true' and DIVERTED = 'false'  \").toPandas()\n",
    "df3 = spark.sql(\"select OUTCOME, ORIGIN_CIG_AVG, DEST_SLP_AVG from airlines_and_weather where CANCELLED = 'false' and DIVERTED = 'true'  \").toPandas()\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax1a = fig.add_subplot(2, 3, 1)\n",
    "ax1b = fig.add_subplot(2, 3, 2)\n",
    "ax1c = fig.add_subplot(2, 3, 3)\n",
    "ax2a = fig.add_subplot(2, 3, 4)\n",
    "ax2b = fig.add_subplot(2, 3, 5)\n",
    "ax2c = fig.add_subplot(2, 3, 6)\n",
    "\n",
    "df1.boxplot(column ='ORIGIN_CIG_AVG', by='OUTCOME', ax=ax1a, showfliers=False)\n",
    "df2.boxplot(column ='ORIGIN_CIG_AVG', ax=ax1b, showfliers=False)\n",
    "df3.boxplot(column ='ORIGIN_CIG_AVG', ax=ax1c, showfliers=False)\n",
    "\n",
    "\n",
    "ax1a.set_ylim(0, 20000)\n",
    "ax1b.set_ylim(0, 20000)\n",
    "ax1c.set_ylim(0, 20000)\n",
    "ax1a.set_title(\"ontime vs delayed\")\n",
    "ax1b.set_title(\"cancelled\")\n",
    "ax1c.set_title(\"diverted\")\n",
    "\n",
    "\n",
    "df1.boxplot(column ='DEST_SLP_AVG', by='OUTCOME', ax=ax2a, showfliers=False)\n",
    "df2.boxplot(column ='DEST_SLP_AVG', ax=ax2b, showfliers=False)\n",
    "df3.boxplot(column ='DEST_SLP_AVG', ax=ax2c, showfliers=False)\n",
    "\n",
    "ax2a.set_ylim(10000, 10300)\n",
    "ax2b.set_ylim(10000, 10300)\n",
    "ax2c.set_ylim(10000, 10300)\n",
    "ax2a.set_title(\"ontime vs delayed\")\n",
    "ax2b.set_title(\"cancelled\")\n",
    "ax2c.set_title(\"diverted\")\n",
    "\n",
    "fig.suptitle('Weather - On time, Delay, Cancelled, Diverted')\n",
    "plt.subplots_adjust(hspace=.5)\n",
    "\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EDA: Are there seasonal and time-of-day trends for delays?\n",
    "\n",
    "We explored temporal factors to determine if there would be value in building a multi-temporal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 EDA: Seasonal trends\n",
    "\n",
    "While flight frequency is relatively steady throughout the year, we noticed that flight delay times varied by 'season', specifically in summer and winter. For this analysis we define the Seasons as:  \n",
    "* **Spring**: March (3) , April (4) , May (5)  \n",
    "* **Summer**: June (6) , July (7) , August (8)\n",
    "* **Fall**: September (9) , October (10) , November (11)\n",
    "* **Winter**: December (12) , January (1) , February (2)  \n",
    "\n",
    "This seaonal cycle could be because of a variety of intuitive factors such as weather, increased holiday travels etc. Since we recongnize there is a clear indicator that delays vary by time of year and thus predicting a flight occuring in the fall on a model that is trained on data from summer may not be optimal. This insight from the EDA has helped inform our decision to use an ensemble model separated in part by seasons for our final algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot delays and trip counts by month\n",
    "delay_by = spark.sql(\"select month, avg(ARR_DELAY_NEW) as delay, count(*) as trip_count from airlines group by month order by month\")\n",
    "result_df = delay_by.select(\"*\").toPandas()\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "sns.barplot(x=\"month\", y=\"delay\", data=result_df, ax=ax1,  dodge=False)\n",
    "sns.barplot(x=\"month\", y=\"trip_count\", data=result_df, ax=ax2,  dodge=False)\n",
    "\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 EDA: Time of day of flight departure\n",
    "\n",
    "Arrival time also appears to have an impact on delay time, with flights arriving later in the evening experiencing the longest delays. Literature (Kafle et all, 2016) suggests that this may be due to a delay propagation, where some delay originating from an upstream flight spreads to downstream flights. In other words, previously delayed flights contribute to the delays in subsequent flights. To that end, this could be a reason why flights arriving later in the day experiecne higher delays. Crucially, as with seasonality, the variabiliy in delay based on time of day motivated us to include arrival time of day as an element in our ensemble model structure. For this analysis we define the Arrival Time of Day as:  \n",
    "* **MORNING**: 0600 <= \"ARR_TIME_BLK\" <= 1359\n",
    "* **EVENING**: 1400 <= \"ARR_TIME_BLK\" <= 2159\n",
    "* **LATE_NIGHT**: 2200 <= \"ARR_TIME_BLK\" <= 0559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA of arrival delay in minutes by time of flight arrival\n",
    "plt.rcParams[\"figure.figsize\"] = (25,6)\n",
    "arrive_delay_df = spark.sql(\"select ARR_DELAY_NEW, ARR_TIME_BLK from airlines where ARR_DELAY_NEW > 0 and ARR_DELAY_NEW < 120\").toPandas()\n",
    "ax1 = arrive_delay_df.boxplot(column=['ARR_DELAY_NEW'], by=['ARR_TIME_BLK'])\n",
    "ax1.set_title('Time of day of flight arrival')\n",
    "ax1.set_xlabel('ARR_TIME_BLK'); ax1.set_ylabel('ARR_DELAY_NEW');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 EDA: Are there continuous features that are highly correlated?\n",
    "\n",
    "This analysis was important in order to determine the risk of multicollinearity within our logistic regression model.  Due to high correlation, we removed the following features:\n",
    "QUARTER, CRS_ARR_TIME, DISTANCE.  \n",
    "\n",
    "Even though they are correlated, we left in TEMP, DEW, and SLP (sea level air pressure) given that seasonality and general weather patterns are critical factors in airline delays. We want to ensure that multi-collinarity doesn't impact our Logistic Regression model, so we did some exploratory runs comparing our metrics for data with weather, data without any weather and data without TEMP.  This exploratory work demonstrated that weather has significant predictive power and that leaving TEMP in the model is well tolerated (see EDA notebook link above for code).\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/correlations_continuous_variables.png?raw=true\" width=\"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Feature Engineering\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/fe.png?raw=true\" width=\"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3.1 Feature selection process\n",
    "\n",
    "Along our EDA, our team adopted a systematic procedure to review and determine which variables should be potential features and which should be excluded, either due to relevance in predicting the outcome and practical considerations of capturing the data in a real-world use case. Details of our collaborative effort to determine which features to include/exclude are provided in [this Google spreadsheet](https://docs.google.com/spreadsheets/d/1O-BqLHvE9cyPvAKEmtWlk9OBhoGCAhXaeJuuIPRCI3o/edit?)\n",
    "\n",
    "From the original 2 datasets, this involved the following:\n",
    "- Airlines dataset: we removed variables that provided \n",
    "  - Duplicative information (i.e. different encodings for the airport station, city, and time variables such as quarter given that we were already including month), \n",
    "  - Variables that were linked too closely to the outcome (i.e. taxi in, taxi out time, and all the delay status variables) and finally \n",
    "  - Variables that based on our EDA and our intuition did not appear to be a meaningful predictor of delays (i.e. day of month, US State of origin or destination airport)\n",
    "- Weather dataset:\n",
    "  - See [separate notebook for joining weather to airlines](https://dbc-b1c912e7-d804.cloud.databricks.com/?o=7564214546094626#notebook/1652976813635466/command/1652976813635476)\n",
    "  - Given a large number of potential variables that all described a certain kind of weather event (i.e. wind, precipitation, or visibility), for simplicity we utilised only the available continuous variables and ignored all potential categorical variables\n",
    "  - We also simplified the available continuous variables by taking a daily average of their output. While we know this means that we applied 'future weather' to predict delays to flights, we justified this on the basis that the real-world use case would use weather forecasts to support predictions and used this future weather as a proxy for these weather forecasts (see Section 1.5 for a more details discussion of this study limitation)\n",
    "  \n",
    "  <img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/join_weather.png?raw=true\" width=\"50%\" >\n",
    "  \n",
    "- Creating new features:\n",
    "  - We created a directed graph of all the flights in the training set and used this to calculate the pagerank of each of the stations, with the hypothesis that the importance of a airport station may be associated with its likelihood of delayed flights. We therefore created 2 new variables for both the origin airport pagerank and the destination airport pagerank. Additional centrality measures were also considered (betweenness and closeness) but due to high correlation between these and pagerank, we chose to focus on just pagerank.\n",
    "  \n",
    "###3.2 Complete list of features included\n",
    "\n",
    "The below is the exhaustive list of features which were included in all algorithms. Most features are self-explanatory, where they are not self-explanatory, a brief description follows. Additionally, all features were numeric with the exception of some which are noted as categorical. \n",
    "\n",
    "- Airport Station Characteristics \n",
    "  - ORIGIN (Origin airport code - categorical), DEST (Destination airport code - categorical), DEST_LATITUDE, DEST_LONGITUDE, DEST_ELEV, ORIGIN_LATITUDE, ORIGIN_LONGITUDE, ORIGIN_ELEV\n",
    "- Flight-specific features\n",
    "  - OP_UNIQUE_CARRIER (Flight company code - categorial), CRS_ELAPSED_TIME (expected duration of flight)\n",
    "- Time-specific features\n",
    "  - YEAR (categorial), MONTH (categorial), DAY_OF_MONTH (categorical), DAY_OF_WEEK (categorical), CRS_DEP_TIME (time of departure in minutes)\n",
    "- Graph-related (airport importance) features\n",
    "  - DEST_PAGERANK, ORIGIN_PAGERANK\n",
    "- Daily average Weather\n",
    "  - ORIGIN_WND_ANGLE_AVG / DEST_WND_ANGLE_AVG (observed wind speed rate in meters per second), \n",
    "  - ORIGIN_WND_SPEED_AVG /  DEST_WND_ANGLE_AVG (observed wind speed rate in meters per second), \n",
    "  - ORIGIN_CIG_AVG / DEST_CIG_AVG (ceiling height in meters), \n",
    "  - ORIGIN_VIS_AVG / DEST_VIS_AVG (visibility distance in meters), \n",
    "  - ORIGIN_TMP_AVG / DEST_TMP_AVG (air temperature in Celsius, factors of 10), \n",
    "  - ORIGIN_DEW_AVG / DEST_DEW_AVG (dew point temperature in Celsius, factors of 10),  \n",
    "  - ORIGIN_SLP_AVG / DEST_SLP_AVG (sea level pressure in Hectopascals, factor by 10), \n",
    "  - ORIGIN_PRECIP_RATE_AVG / DEST_PRECIP_RATE_AVG (depth of liquid precipitation at time of observation, in millimeters),\n",
    "  - ORIGIN_SNOW_DEPTH_AVG / DEST_SNOW_DEPTH_AVG (snow depth in millimeters)\n",
    "\n",
    "Additionally, for our tree-based models, we were able to incorporate additional features that were highly correlated with some of the features above. This is because tree-based models are not limited by multicollinearity concerns, so by including these features we can potentially identify the best potential predictor features. Additionally, categorical features were transformed to ordinal features using Brieman's methods and are noted accordingly below (as above, all other features are continuous).  \n",
    "\n",
    "- Airport Station Characteristics\n",
    "  - ORIGIN STATE FIPS / DEST STATE FIPS (US State identifier - Ordinal)\n",
    "  - ORIGIN CITY MARKET ID / DEST CITY MARKET ID (US City identifier - Ordinal)\n",
    "- Flight-specific features\n",
    "  - CRS_ARR_TIME\n",
    "  - DISTANCE\n",
    "- Time-specific features\n",
    "  - QUARTER, CRS_ARR_TIME (expected arrival time)\n",
    "  \n",
    "###3.3 Missing values treatment and Encoding process\n",
    "\n",
    "- Missing values. We spent considerable effort recovering all of the weather stations (except for one ambiguous airport in North Dakota) and we selected only those weather fields that were core, mandatory fields from the weather data collection.  In addition, we included one optional weather field, the snow depth, since it was approximately 75% non-null. (We set the value to zero for the 25% missing snow depth entries.)  The query provided below **(see 3.3.1)**, shows the percent of missing data for each feature.  Outside of snow depth, other features were present in **99.5%** of the observations, so we choose to skip those rows (handleInvalid=\"skip\" in data encoding pipeline) with missing values. This resulted in minimal dropout from 31,746,841 observations down to 30,999,177\n",
    " observations (2.3% dropout).\n",
    "\n",
    "- Treatment of categorical variables (one hot encoding). For logistic regression, we one-hot encoded several categorical features.  However, for decision trees algorithms (including Random Forest and Gradient Boosted Trees), we created a separate data encoding pipeline, treating ordered categorical features (e.g. DAY_OF_WEEK, QUARTER, DAY_OF_MONTH) as numerical and converting unordered categorical features (e.g. origin and destination airports and states, airlines carrier id, etc) to ordinals using Brieman's method.\n",
    "\n",
    "- Standardization. For the logistic regression data encoding pipeline, all numeric features were standardized.  This is necessary given that some features in terms of their mean and standard deviation vary wildly from other features depending on how they were measured.  For example, wind speed rate (WND) ranges from 0 to 80 meters per second where as ceiling height (CIG), measured in meters, ranges from 0 to 20,000. Without standardization, gradient descent would likely suffer from non-convergence given that the highest magnitude of values will distort the direction of the gradient.  Also, for L1 and L2 regularization, standardization is critical; otherwise regularization will tend to penalize those features in smaller scales.\n",
    "\n",
    "- Two Data Encoding Pipelines.  Decision trees don't require standardization of numeric values and can handle missing data. Also, given that one-hot encoding removes important signal related to the categorical features makes it unlikely that such features will be considered at a top level node, we decided to create a different data encoding pipeline for decision trees that would forgo standardization, treat ordinal categorical features as numeric, and convert unordered categorical features to ordinals using Brieman's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Create the binary outcome variable\n",
    "#\n",
    "data_raw = airlines_and_weather.withColumn('OUTCOME', f.when((f.col(\"ARR_DELAY_NEW\") > 15) | \\\n",
    "                                          (f.col(\"CANCELLED\") == \"true\") | \\\n",
    "                                          (f.col(\"DIVERTED\") == \"true\"), 1)\\\n",
    "                                          .otherwise(0))\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add id\n",
    "data_raw = data_raw.withColumn('id', f.monotonically_increasing_id())\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Build graph to calculate pagerank on just the data_raw_train dataset (not on unseen data)\n",
    "#\n",
    "import networkx as nx\n",
    "data_raw_train    = spark.sql(\"select * from data_raw where YEAR < 2019 or (YEAR = 2019 and MONTH = 1)\")\n",
    "# create the airline graph as a list of nodes and edges (this has no weights, meaning we are looking at unique paths and not frequency of flights)\n",
    "AIRLINE_GRAPH = {'nodes': data_raw_train.select('ORIGIN', 'DEST').rdd.flatMap(list).distinct().collect(),\n",
    "             'edges': data_raw_train.select('ORIGIN', 'DEST').rdd.map(tuple).collect()}\n",
    "\n",
    "# create directed graph object\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(AIRLINE_GRAPH['nodes'])\n",
    "G.add_edges_from(AIRLINE_GRAPH['edges'])\n",
    "\n",
    "# Spark Dataframe of all stations and their caculated pagerank\n",
    "calculated_rank = nx.pagerank(G, alpha=0.85)\n",
    "df_pagerank = pd.DataFrame(calculated_rank.items(), columns=['Station', 'PageRank'])\n",
    "df_pagerank = spark.createDataFrame(df_pagerank)\n",
    "df_pagerank.createOrReplaceTempView(\"df_pagerank\")\n",
    "\n",
    "# create two new features ORIGIN_PAGERANK and DEST_PAGERANK \n",
    "data_raw = spark.sql(\"select * from data_raw left join df_pagerank on data_raw.DEST == df_pagerank.Station\").drop('Station')\n",
    "data_raw = data_raw.withColumnRenamed('PageRank','DEST_PAGERANK')\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")\n",
    "data_raw = spark.sql(\"select * from data_raw left join df_pagerank on data_raw.ORIGIN == df_pagerank.Station\").drop('Station')\n",
    "data_raw = data_raw.withColumnRenamed('PageRank','ORIGIN_PAGERANK')\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>column_name</th><th>count_missing</th><th>percent_missing</th></tr></thead><tbody><tr><td>ORIGIN_LATITUDE</td><td>8895</td><td>0.03</td></tr><tr><td>ORIGIN_LONGITUDE</td><td>8895</td><td>0.03</td></tr><tr><td>ORIGIN_ELEV</td><td>8895</td><td>0.03</td></tr><tr><td>DEST_LATITUDE</td><td>8899</td><td>0.03</td></tr><tr><td>DEST_LONGITUDE</td><td>8899</td><td>0.03</td></tr><tr><td>DEST_ELEV</td><td>8899</td><td>0.03</td></tr><tr><td>CRS_ELAPSED_TIME</td><td>164</td><td>0.0</td></tr><tr><td>ORIGIN_WND_ANGLE_AVG</td><td>140124</td><td>0.44</td></tr><tr><td>ORIGIN_WND_SPEED_AVG</td><td>139428</td><td>0.44</td></tr><tr><td>ORIGIN_CIG_AVG</td><td>139684</td><td>0.44</td></tr><tr><td>ORIGIN_VIS_AVG</td><td>139732</td><td>0.44</td></tr><tr><td>ORIGIN_TMP_AVG</td><td>135230</td><td>0.43</td></tr><tr><td>ORIGIN_DEW_AVG</td><td>140653</td><td>0.44</td></tr><tr><td>ORIGIN_SLP_AVG</td><td>289247</td><td>0.91</td></tr><tr><td>DEST_WND_ANGLE_AVG</td><td>140133</td><td>0.44</td></tr><tr><td>DEST_WND_SPEED_AVG</td><td>139435</td><td>0.44</td></tr><tr><td>DEST_CIG_AVG</td><td>139692</td><td>0.44</td></tr><tr><td>DEST_VIS_AVG</td><td>139740</td><td>0.44</td></tr><tr><td>DEST_TMP_AVG</td><td>135236</td><td>0.43</td></tr><tr><td>DEST_DEW_AVG</td><td>140662</td><td>0.44</td></tr><tr><td>DEST_SLP_AVG</td><td>289340</td><td>0.91</td></tr><tr><td>ORIGIN_PRECIP_RATE_AVG</td><td>292709</td><td>0.92</td></tr><tr><td>ORIGIN_SNOW_DEPTH_AVG</td><td>8339151</td><td>26.27</td></tr><tr><td>DEST_PRECIP_RATE_AVG</td><td>292762</td><td>0.92</td></tr><tr><td>DEST_SNOW_DEPTH_AVG</td><td>8339052</td><td>26.27</td></tr><tr><td>DEST_PAGERANK</td><td>3174</td><td>0.01</td></tr><tr><td>ORIGIN_PAGERANK</td><td>3169</td><td>0.01</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Compute percent of missing data for each feature\n",
    "#\n",
    "numericCols = [\n",
    "'ORIGIN_LATITUDE','ORIGIN_LONGITUDE', 'ORIGIN_ELEV',\n",
    "'DEST_LATITUDE',  'DEST_LONGITUDE',   'DEST_ELEV',\n",
    "'CRS_ELAPSED_TIME','CRS_DEP_TIME',\n",
    "'ORIGIN_WND_ANGLE_AVG', 'ORIGIN_WND_SPEED_AVG', 'ORIGIN_CIG_AVG', 'ORIGIN_VIS_AVG', 'ORIGIN_TMP_AVG', 'ORIGIN_DEW_AVG', 'ORIGIN_SLP_AVG', \n",
    "'DEST_WND_ANGLE_AVG',   'DEST_WND_SPEED_AVG',   'DEST_CIG_AVG',   'DEST_VIS_AVG',   'DEST_TMP_AVG',   'DEST_DEW_AVG',   'DEST_SLP_AVG',\n",
    "'ORIGIN_PRECIP_RATE_AVG', 'ORIGIN_SNOW_DEPTH_AVG',\n",
    "'DEST_PRECIP_RATE_AVG',   'DEST_SNOW_DEPTH_AVG',\n",
    "'DEST_PAGERANK',\n",
    "'ORIGIN_PAGERANK']\n",
    "null_counts = data_raw.select([count(when(col(c).isNull(), c)).alias(c) for c in numericCols])\n",
    "null_counts_df = null_counts.toPandas()\n",
    "null_counts_df = null_counts_df.loc[:, (null_counts_df > 0).all(axis=0)]\n",
    "total_count = data_raw.count()\n",
    "cols = null_counts_df.columns\n",
    "df = null_counts_df.T\n",
    "df.columns = ['count_missing']\n",
    "df['column_name'] = cols\n",
    "df['percent_missing'] = np.round((df['count_missing'] / total_count) * 100, 2)\n",
    "df = df.reindex(columns=['column_name','count_missing', 'percent_missing'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Zero missing snow depth average\n",
    "#\n",
    "data_raw = data_raw.fillna({'ORIGIN_SNOW_DEPTH_AVG':0})\n",
    "data_raw = data_raw.fillna({'DEST_SNOW_DEPTH_AVG':0})\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2  Data Prep Pipeline for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Encode the data\n",
    "#\n",
    "def create_encoding_stages_lr():\n",
    "  stages = [] # stages in our Pipeline\n",
    "\n",
    "  # Convert label into label indices using the StringIndexer\n",
    "  label_stringIdx = StringIndexer(inputCol=\"OUTCOME\", outputCol=\"label\")\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  # Perform one-hot encoding on categorical features\n",
    "  categoricalColumns = ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST']\n",
    "  for categoricalCol in categoricalColumns:\n",
    "      # Category Indexing with StringIndexer\n",
    "      stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\",  handleInvalid=\"skip\")\n",
    "      # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "      # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "      encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "      # Add stages.  These are not run here, but will run all at once later on.\n",
    "      stages += [stringIndexer, encoder]\n",
    "\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalColumns] \n",
    "  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features_categorical\",  handleInvalid=\"skip\")\n",
    "  stages += [assembler]    \n",
    "\n",
    "  # Scale numeric features\n",
    "  numericCols = [\n",
    "  'ORIGIN_LATITUDE','ORIGIN_LONGITUDE', 'ORIGIN_ELEV',\n",
    "  'DEST_LATITUDE',  'DEST_LONGITUDE',   'DEST_ELEV',\n",
    "  'CRS_ELAPSED_TIME','CRS_DEP_TIME',\n",
    "  'ORIGIN_WND_ANGLE_AVG', 'ORIGIN_WND_SPEED_AVG', 'ORIGIN_CIG_AVG', 'ORIGIN_VIS_AVG', 'ORIGIN_TMP_AVG', 'ORIGIN_DEW_AVG', 'ORIGIN_SLP_AVG', \n",
    "  'DEST_WND_ANGLE_AVG',   'DEST_WND_SPEED_AVG',   'DEST_CIG_AVG',   'DEST_VIS_AVG',   'DEST_TMP_AVG',   'DEST_DEW_AVG',   'DEST_SLP_AVG',\n",
    "  'ORIGIN_PRECIP_RATE_AVG', 'ORIGIN_SNOW_DEPTH_AVG',\n",
    "  'DEST_PRECIP_RATE_AVG',   'DEST_SNOW_DEPTH_AVG',\n",
    "  'DEST_PAGERANK',\n",
    "  'ORIGIN_PAGERANK']\n",
    "  assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features_numeric\",  handleInvalid=\"skip\")\n",
    "  scaler_encoder = StandardScaler(inputCol=\"features_numeric\", outputCol=\"features_scaled\",\n",
    "                          withStd=True, withMean=True)\n",
    "  stages += [assembler, scaler_encoder]\n",
    "\n",
    "  # Assemble categorical (one-hot-encoded features) and scaled numeric features\n",
    "  final_assembler = VectorAssembler(inputCols=[\"features_categorical\", \"features_scaled\"], outputCol=\"features\",  handleInvalid=\"skip\")\n",
    "  stages += [final_assembler]\n",
    "  \n",
    "  return (stages, categoricalColumns + numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">data raw train 24,908,789\n",
       "data raw test  6,838,052\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Cast columns to appropriate data type\n",
    "#\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")\n",
    "data_raw = data_raw.withColumn(\"DEST_LATITUDE\", data_raw[\"DEST_LATITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DEST_LONGITUDE\", data_raw[\"DEST_LONGITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DEST_ELEV\", data_raw[\"DEST_ELEV\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_LATITUDE\", data_raw[\"ORIGIN_LATITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_LONGITUDE\", data_raw[\"ORIGIN_LONGITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_ELEV\", data_raw[\"ORIGIN_ELEV\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"CRS_DEP_TIME\", data_raw[\"CRS_DEP_TIME\"].cast('double'))\n",
    "\n",
    "# cast 'categories' to be one-hote encoded into strings\n",
    "data_raw = data_raw.withColumn(\"YEAR\", data_raw[\"YEAR\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"MONTH\", data_raw[\"MONTH\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DAY_OF_MONTH\", data_raw[\"DAY_OF_MONTH\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DAY_OF_WEEK\", data_raw[\"DAY_OF_WEEK\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"OP_UNIQUE_CARRIER\", data_raw[\"OP_UNIQUE_CARRIER\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN\", data_raw[\"ORIGIN\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DEST\", data_raw[\"DEST\"].cast('string'))\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")\n",
    "\n",
    "#\n",
    "# Split into train and test\n",
    "#\n",
    "data_raw_train    = spark.sql(\"select * from data_raw where YEAR < 2019 or (YEAR = 2019 and MONTH = 1)\")\n",
    "data_raw_test     = spark.sql(\"select * from data_raw where YEAR = 2019 and MONTH > 1\")\n",
    "print('data raw train', f'{data_raw_train.count():,}')\n",
    "print('data raw test ', f'{data_raw_test.count():,}')\n",
    "\n",
    "data_raw_train.createOrReplaceTempView(\"data_raw_train\")\n",
    "data_raw_test.createOrReplaceTempView(\"data_raw_test\")\n",
    "\n",
    "#\n",
    "# Run Pipeline to encode data for logistic regression\n",
    "#\n",
    "# Create pipeline for one-hot encoding, standardizing, and creating label for outcome variable\n",
    "stages, orig_cols = create_encoding_stages_lr()\n",
    "data_prep_pipeline       = Pipeline().setStages(stages)\n",
    "# Fit based on the training data\n",
    "data_prep_model          = data_prep_pipeline.fit(data_raw_train)\n",
    "# Transform (encode) all of the raw data\n",
    "data_prepped             = data_prep_model.transform(data_raw)\n",
    "data_prepped.count()\n",
    "data_prepped.createOrReplaceTempView('data_prepped')\n",
    "\n",
    "#\n",
    "# Split the encoded data into train, test\n",
    "#\n",
    "data_prepped_train    = spark.sql(\"select * from data_prepped where YEAR < 2019 or (YEAR = 2019 and MONTH = 1)\")\n",
    "data_prepped_test     = spark.sql(\"select * from data_prepped where YEAR = 2019 and MONTH > 1\")\n",
    "\n",
    "#\n",
    "# Keep relevant columns\n",
    "#\n",
    "selected_cols = [\"label\", \"features\", \"OUTCOME\", \"id\"] + orig_cols \n",
    "data_prepped_train          = data_prepped_train.select(selected_cols)\n",
    "data_prepped_test           = data_prepped_test.select(selected_cols)\n",
    "\n",
    "#\n",
    "# Separate train and test into smaller subsets\n",
    "#\n",
    "data_train          = data_prepped_train\n",
    "\n",
    "# Taking 50% of both 0,1 outcomes from test to create test data set.  this is our hold-out set\n",
    "data_test         = data_prepped_test.sampleBy(\"outcome\", fractions={0: 0.50, 1: 0.50}, seed=10)\n",
    "\n",
    "# Remaining 50% (unique set) will be our dev set for performance tuning\n",
    "data_dev          = data_prepped_test.subtract(data_test)\n",
    "\n",
    "# Take 5% sample of train for train_small\n",
    "data_train_small    = data_train.sampleBy(\"outcome\", fractions={0: 0.05, 1: 0.05}, seed=10)\n",
    "\n",
    "# Take 5% of from dev to for dev small\n",
    "data_dev_small    = data_dev.sampleBy(\"outcome\", fractions={0: 0.05, 1: 0.05}, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Data Pipeline for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Determine the frequency of each category (sum of outcome / total observations)\n",
    "#\n",
    "def calculate_ordinal(categorical_col, total_count):\n",
    "  ordinal_col    = categorical_col + \"_ORDINAL\"\n",
    "  id_col         = categorical_col + \"_X\"\n",
    "  count_col      = categorical_col + \"_COUNT\"\n",
    "  \n",
    "  the_sql = f\"\"\"select ROW_NUMBER() OVER(ORDER BY count(*) ASC) AS {ordinal_col}, \n",
    "  {categorical_col} AS {id_col}, \n",
    "  sum(OUTCOME) / {total_count} as {count_col}\n",
    "  from data_raw\n",
    "  group by {id_col}\n",
    "  order by {count_col}\"\"\"\n",
    "  \n",
    "  df_xref = spark.sql(the_sql)\n",
    "\n",
    "  data_raw1 = data_raw.join(df_xref, data_raw[categorical_col] == df_xref[id_col], how=\"left\")\n",
    "  data_raw1 = data_raw1.drop(count_col)\n",
    "  data_raw1 = data_raw1.drop(id_col)\n",
    "  \n",
    "  return data_raw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_encoding_stages_dt():\n",
    "  #\n",
    "  # Encode the data\n",
    "  #\n",
    "  stages = [] # stages in our Pipeline\n",
    "\n",
    "  # Convert label into label indices using the StringIndexer\n",
    "  label_stringIdx = StringIndexer(inputCol=\"OUTCOME\", outputCol=\"label\")\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  # ordinal features\n",
    "  ordinalCols= ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'QUARTER', 'OP_UNIQUE_CARRIER_ORDINAL', 'ORIGIN_ORDINAL', 'DEST_ORDINAL', 'ORIGIN_STATE_FIPS_ORDINAL',\n",
    "                'ORIGIN_CITY_MARKET_ID_ORDINAL', 'DEST_STATE_FIPS_ORDINAL', 'DEST_CITY_MARKET_ID_ORDINAL']\n",
    "  #  numeric features\n",
    "  numericCols = [\n",
    "  'DEST_LATITUDE', 'DEST_LONGITUDE', 'DEST_ELEV',\n",
    "  'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE', 'ORIGIN_ELEV',\n",
    "  'CRS_ELAPSED_TIME','CRS_DEP_TIME',\n",
    "  'ORIGIN_WND_ANGLE_AVG', 'ORIGIN_WND_SPEED_AVG', 'ORIGIN_CIG_AVG', 'ORIGIN_VIS_AVG', 'ORIGIN_TMP_AVG', 'ORIGIN_DEW_AVG', 'ORIGIN_SLP_AVG',\n",
    "  'DEST_WND_ANGLE_AVG',   'DEST_WND_SPEED_AVG',   'DEST_CIG_AVG',   'DEST_VIS_AVG',   'DEST_TMP_AVG',   'DEST_DEW_AVG',   'DEST_SLP_AVG',\n",
    "  'ORIGIN_PRECIP_RATE_AVG', 'ORIGIN_SNOW_DEPTH_AVG',\n",
    "  'DEST_PRECIP_RATE_AVG',   'DEST_SNOW_DEPTH_AVG',\n",
    "  'CRS_ARR_TIME','DISTANCE',\n",
    "  'DEST_PAGERANK', 'ORIGIN_PAGERANK']\n",
    "\n",
    "  input_cols = []\n",
    "  input_cols += ordinalCols\n",
    "  input_cols += numericCols\n",
    "  assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "  stages += [assembler]\n",
    "  \n",
    "  return (stages, input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">data raw train 24,908,789\n",
       "data raw test  6,838,052\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Cast columns to appropriate data type\n",
    "#\n",
    "# Cast numerics to doubles\n",
    "data_raw = data_raw.withColumn(\"DEST_LATITUDE\", data_raw[\"DEST_LATITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DEST_LONGITUDE\", data_raw[\"DEST_LONGITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DEST_ELEV\", data_raw[\"DEST_ELEV\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_LATITUDE\", data_raw[\"ORIGIN_LATITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_LONGITUDE\", data_raw[\"ORIGIN_LONGITUDE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_ELEV\", data_raw[\"ORIGIN_ELEV\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"CRS_DEP_TIME\", data_raw[\"CRS_DEP_TIME\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"CRS_ARR_TIME\", data_raw[\"CRS_ARR_TIME\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DISTANCE\", data_raw[\"DISTANCE\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"YEAR\", data_raw[\"YEAR\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"MONTH\", data_raw[\"MONTH\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DAY_OF_MONTH\", data_raw[\"DAY_OF_MONTH\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"DAY_OF_WEEK\", data_raw[\"DAY_OF_WEEK\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"QUARTER\", data_raw[\"QUARTER\"].cast('double'))\n",
    "data_raw = data_raw.withColumn(\"MONTH\", data_raw[\"MONTH\"].cast('double'))\n",
    "# cast 'categories' to be one-hote encoded into strings\n",
    "data_raw = data_raw.withColumn(\"OP_UNIQUE_CARRIER\", data_raw[\"OP_UNIQUE_CARRIER\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN\", data_raw[\"ORIGIN\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DEST\", data_raw[\"DEST\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_STATE_FIPS\", data_raw[\"ORIGIN_STATE_FIPS\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"ORIGIN_CITY_MARKET_ID\", data_raw[\"ORIGIN_CITY_MARKET_ID\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DEST_STATE_FIPS\", data_raw[\"DEST_STATE_FIPS\"].cast('string'))\n",
    "data_raw = data_raw.withColumn(\"DEST_CITY_MARKET_ID\", data_raw[\"DEST_CITY_MARKET_ID\"].cast('string'))\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#  Use Brieman's method to create ordinals for unordered categorical data\n",
    "#\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")\n",
    "total_count = data_raw.count()\n",
    "data_raw = calculate_ordinal('OP_UNIQUE_CARRIER', total_count)\n",
    "data_raw = calculate_ordinal('ORIGIN_STATE_FIPS', total_count)\n",
    "data_raw = calculate_ordinal('DEST_STATE_FIPS', total_count)\n",
    "data_raw = calculate_ordinal('ORIGIN_CITY_MARKET_ID', total_count)\n",
    "data_raw = calculate_ordinal('DEST_CITY_MARKET_ID', total_count)\n",
    "data_raw = calculate_ordinal('DEST', total_count)\n",
    "data_raw = calculate_ordinal('ORIGIN', total_count)\n",
    "data_raw.createOrReplaceTempView(\"data_raw\")\n",
    "\n",
    "#\n",
    "# Split into train and test\n",
    "#\n",
    "data_raw_train    = spark.sql(\"select * from data_raw where YEAR < 2019 or (YEAR = 2019 and MONTH = 1)\")\n",
    "data_raw_test     = spark.sql(\"select * from data_raw where YEAR = 2019 and MONTH > 1\")\n",
    "print('data raw train', f'{data_raw_train.count():,}')\n",
    "print('data raw test ', f'{data_raw_test.count():,}')\n",
    "\n",
    "data_raw_train.createOrReplaceTempView(\"data_raw_train\")\n",
    "data_raw_test.createOrReplaceTempView(\"data_raw_test\")\n",
    "\n",
    "#\n",
    "# Run Pipeline to encode data for logistic regression\n",
    "#\n",
    "\n",
    "# Create pipeline for one-hot encoding, standardizing, and creating label for outcome variable\n",
    "stages, orig_cols        = create_encoding_stages_dt()\n",
    "data_prep_pipeline       = Pipeline().setStages(stages)\n",
    "# Fit based on the training data\n",
    "data_prep_model          = data_prep_pipeline.fit(data_raw_train)\n",
    "# Transform (encode) all of the raw data\n",
    "data_prepped_dt          = data_prep_model.transform(data_raw)\n",
    "data_prepped_dt.count()\n",
    "data_prepped_dt.createOrReplaceTempView('data_prepped_dt')\n",
    "\n",
    "\n",
    "#\n",
    "# Split the encoded data into train, test\n",
    "#\n",
    "data_prepped_train_dt    = spark.sql(\"select * from data_prepped_dt where YEAR < 2019 or (YEAR = 2019 and MONTH = 1)\")\n",
    "data_prepped_test_dt     = spark.sql(\"select * from data_prepped_dt where YEAR = 2019 and MONTH > 1\")\n",
    "\n",
    "#\n",
    "# Keep relevant columns\n",
    "#\n",
    "selected_cols = [\"label\", \"features\", \"OUTCOME\", \"id\"] + orig_cols\n",
    "data_prepped_train_dt          = data_prepped_train_dt.select(selected_cols)\n",
    "data_prepped_test_dt           = data_prepped_test_dt.select(selected_cols)\n",
    "\n",
    "data_prepped_train_dt.createOrReplaceTempView('data_prepped_train_dt')\n",
    "data_prepped_test_dt.createOrReplaceTempView('data_prepped_test_dt')\n",
    "\n",
    "#\n",
    "# Separate train and test into smaller subsets\n",
    "#\n",
    "data_train_tree          = data_prepped_train_dt\n",
    "\n",
    "# Taking 50% of both 0,1 outcomes from test to create test data set.  this is our hold-out set\n",
    "data_test_tree         = data_prepped_test_dt.sampleBy(\"outcome\", fractions={0: 0.50, 1: 0.50}, seed=10)\n",
    "\n",
    "# Remaining 50% (unique set) will be our dev set for performance tuning\n",
    "data_dev_tree          = data_prepped_test_dt.subtract(data_test_tree)\n",
    "\n",
    "# Take 5% sample of train for train_small\n",
    "data_train_small_tree    = data_train_tree.sampleBy(\"outcome\", fractions={0: 0.05, 1: 0.05}, seed=10)\n",
    "\n",
    "# Take 5% of from dev to for dev small\n",
    "data_dev_small_tree    = data_dev_tree.sampleBy(\"outcome\", fractions={0: 0.05, 1: 0.05}, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Algorithm Exploration\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/algo_explore1.png?raw=true\" width=\"60%\" >\n",
    "\n",
    "For our analysis, we selected 2 traditional classification algorithms, specifically logistic regression and decision trees, with the intention to further consider various regularization options for logistic regression and bagging, boosting and random forest options for decision trees. Due to the fact that logistic regression is not suitable in the case of multicollinearity, our EDA efforts noted above were specifically designed to ensure the data pipeline informing our logistic regression model removed features which were highly correlated. We also ensured that our continuous features were normalized with mean = 0 and standard deviation = 1 so that gradient descent process would be optimized. \n",
    "\n",
    "For our decision tree-based models, we were able to incorporate more features without concern about multicollinearity and generally include all features with limited preprocessing (no normalization or one-hot encoding of categorical features). Our expectation is that ensemble decision tree methods (specifically bagging with random forest and bagging with gradient-boosted trees) would produce the strongest results, as these models train on the most data, use balanced training sets, handle missing values more easily, and balance the bias-variance tradeoff through ensembles. In terms of computational cost (or time required), logistic regression and decision trees should be the most performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.1 Logistic Regression\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/lr.png?raw=true\" align=\"left\" width=\"10%\" >\n",
    "\n",
    "\n",
    "For logistic regression, we first started with a simple model trained on the full dataset. We then proceeded to manually explore L1 and L2 regularization (code not included for brevity). We used a gridsearch and crossfold validation to further explore hyperparameter tuning. After some initial attempts to perform these operations on the full training set, we concluded that we would need to sample in order to perform the crossfold validation in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to evaluate model performance\n",
    "\n",
    "We were most interested in recall and F1 as our primary performance metrics. These metrics were not accessible via the API, so we wrote a custom function to calculate the true positive, true negative, false positive, and false negative counts needed to calculate recall and F1. We also monitored the area under the precision-recall curve, or AUC PR. This metric closely followed F1 and was accessible from the model object returned from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataframe to store final results\n",
    "predictionDFcolumns =['accuracy','precision', 'recall', 'f1score', 'areaUnderPR', 'tp', 'tn', 'fp', 'fn']\n",
    "predictionDF = pd.DataFrame(columns=predictionDFcolumns)\n",
    "# Save results to csv\n",
    "metrics_filename = \"/dbfs/user/ammara.essa@ischool.berkeley.edu/W261_Final_Project/predictions_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluatePerformance(predictions, modelName = 'Test'):\n",
    "  \"\"\"\n",
    "  Function that takes in a dataframe that includes a column of 'labels' and 'predictions' and\n",
    "  parses these labels and predictions to calculate, print, and return as a list the suite of accuracy measures\n",
    "  we are interested in. \"model_name\" is a string that provides the model name and is used for saving results to dataframe.\n",
    "  \"\"\"\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "  # manual counts\n",
    "  tp = spark.sql(\"select count(*) from predictions where label = 1 and prediction = 1\").first()[0]\n",
    "  tn = spark.sql(\"select count(*) from predictions where label = 0 and prediction = 0\").first()[0]\n",
    "  fp = spark.sql(\"select count(*) from predictions where label = 0 and prediction = 1\").first()[0]\n",
    "  fn = spark.sql(\"select count(*) from predictions where label = 1 and prediction = 0\").first()[0]\n",
    "  \n",
    "\n",
    "  # Overall statistics\n",
    "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "  recall = tp / (tp+fn)\n",
    "  \n",
    "  precision = 0\n",
    "  if tp+fp != 0:\n",
    "    precision = tp / (tp+fp)\n",
    "\n",
    "  f1score = 0\n",
    "  if recall + precision != 0:\n",
    "    f1score = 2*recall*precision/(recall + precision)\n",
    "\n",
    "  \n",
    "  # Area under precision-recall curve\n",
    "  pl_rdd = predictions.select(predictions.label.cast(\"float\"), predictions.prediction.cast(\"float\")).rdd\n",
    "  pr_metrics = BinaryClassificationMetrics(pl_rdd)\n",
    "  auc = pr_metrics.areaUnderPR\n",
    "\n",
    "  print(\"Summary Stats\")\n",
    "  print(\"tp (correct   pred as delay) \\t%s\" % f'{tp:,}')\n",
    "  print(\"tn (correct   pred as ontime)\\t%s\" % f'{tn:,}')\n",
    "  print(\"fp (incorrect pred as delay) \\t%s\" % f'{fp:,}')\n",
    "  print(\"fn (incorrect pred as ontime)\\t%s\" % f'{fn:,}, \"\\n\"')\n",
    "\n",
    "  print(\"accuracy \\t%s\"  % np.round(accuracy,4),  \"\\tout of all observations, how many were predicted correctly?\")\n",
    "  print(\"precision\\t%s\"  % np.round(precision,4), \"\\tout of all delays predicted, how many are correct?\")\n",
    "  print(\"recall   \\t%s\"  % np.round(recall,4),    \"\\tout of all actual delays, how many are correctly predicted?\")\n",
    "  print(\"f1       \\t%s\"  % np.round(f1score,4))\n",
    "  print(\"Area under PR = %s\" % auc)\n",
    "  \n",
    "  #Store the metrics in a dataframe to return & write output to csv file\n",
    "#   predictionDF = pd.read_csv(metrics_filename, index_col=0)\n",
    "  predictionDF.loc[modelName] = [accuracy, precision, recall, f1score, auc, tp, tn, fp, fn]\n",
    "#   predictionDF.to_csv(metrics_filename)\n",
    "  \n",
    "  #reset the predictions for the next iteration\n",
    "  predictions = tp = tn = fp = fn = None\n",
    "\n",
    "# Function to show curves\n",
    "def showCurve(modelSummary):\n",
    "  \"\"\"\n",
    "  Function that takes in the modelSummary object from a trained model and calls \n",
    "  two additional plot functions to return Area under the ROC and Precision/Recall curves\n",
    "  \"\"\"\n",
    "  fig = plt.figure(figsize=(10,5))\n",
    "  ax1 = fig.add_subplot(1, 2, 1)\n",
    "  ax2 = fig.add_subplot(1, 2, 2)\n",
    "  showROCCurve(modelSummary, ax1)\n",
    "  showPRCurve(modelSummary, ax2)\n",
    "  display(plt.show())\n",
    "  \n",
    "def showROCCurve(modelSummary, ax):\n",
    "  roc = modelSummary.roc.toPandas()\n",
    "  ax.plot(roc['FPR'],roc['TPR'])\n",
    "  ax.set_ylabel('False Positive Rate')\n",
    "  ax.set_xlabel('True Positive Rate')\n",
    "  ax.set_title('ROC Curve')\n",
    "\n",
    "def showPRCurve(modelSummary, ax):\n",
    "  pr = modelSummary.pr.toPandas()\n",
    "  ax.plot(pr['recall'],pr['precision'])\n",
    "  ax.set_xlabel('Precision')\n",
    "  ax.set_ylabel('Recall')\n",
    "  ax.set_title('Precision Recall')\n",
    "  \n",
    "# Function to show confusion matrix\n",
    "def showConfusionMatrix(modelSummary):\n",
    "  \"\"\"\n",
    "  Function that takes in the modelSummary object from a trained model (where the tp, tn, fp, fn have already been calculated\n",
    "  and returns a confusion matrix in the form of a dataframe\n",
    "  \"\"\"\n",
    "  confMatrix = pd.DataFrame(columns=['Delay_(Predicted)','No_Delay_(Predicted)'])\n",
    "  confMatrix.loc['Delay_(Actual)','Delay_(Predicted)'] = modelSummary['tp']\n",
    "  confMatrix.loc['No_Delay_(Actual)','No_Delay_(Predicted)'] = modelSummary['tn']\n",
    "  confMatrix.loc['No_Delay_(Actual)','Delay_(Predicted)'] = modelSummary['fp']\n",
    "  confMatrix.loc['Delay_(Actual)','No_Delay_(Predicted)'] = modelSummary['fn']\n",
    "  cm = sns.light_palette(\"blue\", as_cmap=True)\n",
    "  s = confMatrix.style.background_gradient(cmap=cm)\n",
    "  display(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to show feature importance\n",
    "\n",
    "One important aspect of logistic regression models is their explainability. By extracting features and feature weights and signs, we can understand the features that have the largest impact on the model's predictons, as well as the direction of impact. We create custom functions to extract this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Show feature importance\n",
    "#\n",
    "def display_feature_importance_lr(sorted_feature_scores):\n",
    "  seen = {}\n",
    "  lookup = {\n",
    "  'features_scaled_0': 'DEST_LATITUDE',\n",
    "  'features_scaled_1': 'DEST_LONGITUDE',\n",
    "  'features_scaled_2': 'DEST_ELEV',\n",
    "  'features_scaled_3': 'ORIGIN_LATITUDE',\n",
    "  'features_scaled_4': 'ORIGIN_LONGITUDE',\n",
    "  'features_scaled_5': 'ORIGIN_ELEV',\n",
    "  'features_scaled_6': 'CRS_ELAPSED_TIME',\n",
    "  'features_scaled_7': 'CRS_DEP_TIME',\n",
    "  'features_scaled_8': 'ORIGIN_WND_ANGLE_AVG',\n",
    "  'features_scaled_9': 'ORIGIN_WND_SPEED_AVG',\n",
    "  'features_scaled_10': 'ORIGIN_CIG_AVG',\n",
    "  'features_scaled_11': 'ORIGIN_VIS_AVG',\n",
    "  'features_scaled_12': 'ORIGIN_TMP_AVG',\n",
    "  'features_scaled_13': 'ORIGIN_DEW_AVG',\n",
    "  'features_scaled_14': 'ORIGIN_SLP_AVG',\n",
    "  'features_scaled_15': 'ORIGIN_PRECIP_RATE_AVG',\n",
    "  'features_scaled_16': 'ORIGIN_SNOW_DEPTH_AVG',\n",
    "  'features_scaled_17': 'DEST_WND_ANGLE_AVG',\n",
    "  'features_scaled_18': 'DEST_WND_SPEED_AVG',\n",
    "  'features_scaled_19': 'DEST_CIG_AVG',\n",
    "  'features_scaled_20': 'DEST_VIS_AVG',\n",
    "  'features_scaled_21': 'DEST_TMP_AVG',\n",
    "  'features_scaled_22': 'DEST_DEW_AVG',\n",
    "  'features_scaled_23': 'DEST_SLP_AVG',\n",
    "  'features_scaled_24': 'DEST_PRECIP_RATE_AVG',\n",
    "  'features_scaled_25': 'DEST_SNOW_DEPTH_AVG',\n",
    "  'features_scaled_26': 'DEST_PAGERANK',\n",
    "  'features_scaled_27': 'ORIGIN_PAGERANK'}\n",
    "  \n",
    "  \n",
    "  for name, score in sorted_feature_scores:\n",
    "    trimmed = name\n",
    "    try:\n",
    "      idx = name.index('classVec_')\n",
    "      if idx >= 0:\n",
    "        trimmed = name[:idx]\n",
    "    except:\n",
    "      trimmed = name\n",
    "    try:\n",
    "      idx = trimmed.index('features_categorical_')\n",
    "      if idx >= 0:\n",
    "         trimmed = trimmed[len('features_categorical_'):]\n",
    "    except:\n",
    "      trimmed = trimmed\n",
    "    if trimmed not in seen:\n",
    "      if trimmed in lookup:\n",
    "        print(lookup[trimmed], np.round(score,3))\n",
    "      else:\n",
    "        print(trimmed, np.round(score,3))\n",
    "      seen[trimmed] = True\n",
    "\n",
    "def determine_feature_importance_lr(predictions):      \n",
    "  features = [x[\"name\"] for x in sorted(predictions.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"binary\"]+ predictions.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"], key=lambda x: x[\"idx\"])]\n",
    "\n",
    "  #features = [ x['name'] for x in predictions.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]]\n",
    "  cf = lrModel.coefficientMatrix.toArray().tolist()[0]\n",
    "\n",
    "  feature_cf =  list(zip(features, cf))\n",
    "  \n",
    "  #\n",
    "  # Feature Importance (ordered by absolute value of coefficient)\n",
    "  #\n",
    "  sorted_feature_scores_abs_class = sorted(feature_cf, key=lambda tup:(-abs(tup[1]), tup[0]))\n",
    "  display_feature_importance_lr(sorted_feature_scores_abs_class)\n",
    "  \n",
    "  \n",
    "def display_feature_importance_tree(model, predictions):\n",
    "  featureImp = model.featureImportances\n",
    "  list_extract = []\n",
    "  for i in predictions.schema['features'].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "      list_extract = list_extract + predictions.schema['features'].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "  varlist = pd.DataFrame(list_extract)\n",
    "  varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "  return varlist.sort_values('score', ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Logistic Regression - Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial exploration of logistic regression is performed on the complete dataset. We have not (yet) taken any actions to balance the dataset between classes and simply accept the default parameters for the model. We explored a number of different combinations of hyperparameters (examples not shown), primarily focusing on comparing regularization routines. The MLLib API has a hyperparameter (ElasticNet) that sets the regularization to either L1 or L2, but it can also blend the two types of regularization. Despite efforts at tuning, we saw little performance improvement when training on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Logistic Regression Model [unweighted] with default parameters </span>\n",
       "\n",
       "Trained model in 2.372539273897807 minutes\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t38,712\n",
       "tn (correct   pred as ontime)\t2,613,418\n",
       "fp (incorrect pred as delay) \t30,435\n",
       "fn (incorrect pred as ontime)\t628,874, &#34;\n",
       "&#34;\n",
       "accuracy \t0.8009 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.5599 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.058 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.1051\n",
       "Area under PR = 0.04982177382321286\n",
       "\n",
       "Evaluated metrics in 0.3620173176129659 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create logistic regression model\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label')\n",
    "print(\"\\033[1m Logistic Regression Model [unweighted] with default parameters \\033[0m\")\n",
    "start = time.time()\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(data_train)\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"Evaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "\n",
    "# Generate predictions on the dev set\n",
    "predictions = lrModel.transform(data_dev).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "\n",
    "# Evaluate the performance of the dev set predictions\n",
    "evaluatePerformance(predictions, 'LR_Unweighted_DefaultParms')\n",
    "\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1score</th>\n",
       "      <th>areaUnderPR</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR_Unweighted_DefaultParms</th>\n",
       "      <td>0.8009</td>\n",
       "      <td>0.559851</td>\n",
       "      <td>0.057988</td>\n",
       "      <td>0.105091</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>2613418.0</td>\n",
       "      <td>30435.0</td>\n",
       "      <td>628874.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample output the dataframe with model results. The model name is the index.\n",
    "pd.DataFrame(predictionDF.loc['LR_Unweighted_DefaultParms']).transpose()\n",
    "# predictionDF.loc['LR_Unweighted_DefaultParms','tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_7fc661e4_80f2_11ea_9380_00163e1d8677row0_col0 {\n",
       "            background-color:  #0000ff;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_7fc661e4_80f2_11ea_9380_00163e1d8677row0_col1 {\n",
       "            background-color:  #e5e5ff;\n",
       "            color:  #000000;\n",
       "        }    #T_7fc661e4_80f2_11ea_9380_00163e1d8677row1_col0 {\n",
       "            background-color:  #e5e5ff;\n",
       "            color:  #000000;\n",
       "        }    #T_7fc661e4_80f2_11ea_9380_00163e1d8677row1_col1 {\n",
       "            background-color:  #0000ff;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Delay_(Predicted)</th>        <th class=\"col_heading level0 col1\" >No_Delay_(Predicted)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677level0_row0\" class=\"row_heading level0 row0\" >Delay_(Actual)</th>\n",
       "                        <td id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677row0_col0\" class=\"data row0 col0\" >38712</td>\n",
       "                        <td id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677row0_col1\" class=\"data row0 col1\" >628874</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677level0_row1\" class=\"row_heading level0 row1\" >No_Delay_(Actual)</th>\n",
       "                        <td id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677row1_col0\" class=\"data row1 col0\" >30435</td>\n",
       "                        <td id=\"T_7fc661e4_80f2_11ea_9380_00163e1d8677row1_col1\" class=\"data row1 col1\" >2613418</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show confusion matrix\n",
    "print('\\033[1m Confusion matrix for model : LR_Unweighted_DefaultParms \\033[0m')\n",
    "showConfusionMatrix(predictionDF.loc['LR_Unweighted_DefaultParms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.1.2 Logistic Regression - with Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalance between delayed and on-time observations in the training set( ~80% ontime / 20 delayed), we employed a technique called Class Weighing. Using this method, we ensure that the logistic loss objective function treats the positive (delayed) class (Outcome == 1) with higher weight. The functions below described how the balancing ratio for the training data is calculated and applied to each observation of the training, dev and test data (in a new column 'weight')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to calculate and assign weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#helper functions to set up the weighted model\n",
    "def getWeight(df):\n",
    "  \"\"\"\n",
    "  Takes in a dataframe and returns class weights\n",
    "  \"\"\"\n",
    "  df.createOrReplaceTempView(\"df\")\n",
    "  outcome_counts_nodelay  = spark.sql(\"select  count(*) from df where OUTCOME = 0\")\n",
    "\n",
    "  outcome_counts_delay    = spark.sql(\"select  count(*) from df where OUTCOME = 1\")\n",
    "\n",
    "  total = spark.sql(\"select  count(*) from df\")\n",
    "\n",
    "  tot     = int(total.first()[0])\n",
    "  nodelay = int(outcome_counts_nodelay.first()[0])\n",
    "  delay   = int(outcome_counts_delay.first()[0])\n",
    "\n",
    "  balancing_ratio = (tot - nodelay) / tot\n",
    "  weight_nodelay  = 1 * balancing_ratio\n",
    "  weight_delay    = 1 * (1 - balancing_ratio)\n",
    "\n",
    "  print(\"no delay\", nodelay, '\\tratio', nodelay/tot, '\\tweight', weight_nodelay)\n",
    "  print(\"delay.  \", delay, '\\tratio', delay/tot, '\\tweight', weight_delay)\n",
    "#   print(\"training set lost records check:\", (data_train_count - tot) )\n",
    "  return (weight_nodelay,weight_delay)\n",
    "\n",
    "def setOutcome(label):\n",
    "  \"\"\"\n",
    "  Takes in a label, returns no_delay_weight to underweight the on time flights if label = 0\n",
    "  \"\"\"\n",
    "  if label == 0: return weight_nodelay\n",
    "  else: return weight_delay\n",
    "\n",
    "\n",
    "def setWeight(df):\n",
    "  \"\"\"\n",
    "  Takes in an unweighted dataframe and returns a dataframe with a weights column added\n",
    "  \"\"\"\n",
    "  udfSetOutcome = udf(setOutcome, DoubleType())\n",
    "  df_with_weight = df.withColumn(\"weight\", udfSetOutcome(\"label\"))\n",
    "  return df_with_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and assign weights to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Class weights calculated from training data </span>\n",
       "no delay 19698189 \tratio 0.8081288083750847 \tweight 0.1918711916249153\n",
       "delay.   4676872 \tratio 0.1918711916249153 \tweight 0.8081288083750847\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate and assign weights to data\n",
    "\n",
    "print('\\033[1m Class weights calculated from training data \\033[0m')\n",
    "weight_nodelay,weight_delay = getWeight(data_train)\n",
    "data_train       = setWeight(data_train)\n",
    "data_train_small = setWeight(data_train_small)\n",
    "\n",
    "data_train.createOrReplaceTempView(\"data_train\")\n",
    "data_train_small.createOrReplaceTempView(\"data_train_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Logistic Regression Model [weighted] with default parameters) </span>\n",
       "\n",
       "Trained model in 2.2814802090326944 minutes\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t417,076\n",
       "tn (correct   pred as ontime)\t1,749,705\n",
       "fp (incorrect pred as delay) \t894,148\n",
       "fn (incorrect pred as ontime)\t250,510, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6543 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3181 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6248 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4215\n",
       "Area under PR = 0.5467462904164218\n",
       "\n",
       "Evaluated metrics in 0.2148897171020508 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create logistic regression model and train on training set, create predictions on the dev set and evaluate performance\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "\n",
    "print(\"\\033[1m Logistic Regression Model [weighted] with default parameters) \\033[0m\")\n",
    "start = time.time()\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(data_train)\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"Evaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "predictions = lrModel.transform(data_dev).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"LR_Weighted_DefaultParms\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to explore feature importance in our weighted logistic regresion model, we then extract the model coefficients.\n",
    "\n",
    "For binary classification, the coefficients represent the odds.  Positive coefficients for a feature indicate that the delay becomes more likely and negative coefficients indicate that the delay becomes less likely.   We sort by absolute value to rank both strongly positive and strongly negative features in terms of their predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\">Feature Coefficients: Logistic Regression Model [weighted] with default parameters) </span>\n",
       "ORIGIN_AIRPORT_SEQ_ID -10.327\n",
       "DEST_AIRPORT_SEQ_ID -9.353\n",
       "MONTH 0.997\n",
       "OP_UNIQUE_CARRIER -0.796\n",
       "CRS_DEP_TIME 0.38\n",
       "DAY_OF_WEEK 0.285\n",
       "DAY_OF_MONTH 0.239\n",
       "ORIGIN_TMP_AVG -0.227\n",
       "DEST_TMP_AVG -0.213\n",
       "ORIGIN_VIS_AVG -0.207\n",
       "DEST_VIS_AVG -0.161\n",
       "DEST_DEW_AVG 0.141\n",
       "DEST_WND_SPEED_AVG 0.106\n",
       "ORIGIN_WND_SPEED_AVG 0.094\n",
       "DEST_CIG_AVG -0.087\n",
       "DEST_PRECIP_RATE_AVG 0.079\n",
       "ORIGIN_PRECIP_RATE_AVG 0.079\n",
       "ORIGIN_CIG_AVG -0.059\n",
       "ORIGIN_DEW_AVG 0.059\n",
       "ORIGIN_PAGERANK 0.051\n",
       "DEST_PAGERANK 0.048\n",
       "ORIGIN_SLP_AVG -0.044\n",
       "YEAR 0.042\n",
       "ORIGIN_LATITUDE -0.025\n",
       "DEST_LATITUDE -0.022\n",
       "DEST_ELEV -0.021\n",
       "ORIGIN_LONGITUDE 0.012\n",
       "ORIGIN_ELEV -0.012\n",
       "ORIGIN_SNOW_DEPTH_AVG 0.01\n",
       "ORIGIN_WND_ANGLE_AVG 0.01\n",
       "DEST_LONGITUDE -0.008\n",
       "DEST_SNOW_DEPTH_AVG 0.006\n",
       "DEST_WND_ANGLE_AVG -0.005\n",
       "DEST_SLP_AVG 0.003\n",
       "CRS_ELAPSED_TIME 0.002\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display feature importance\n",
    "print(\"\\033[1mFeature Coefficients: Logistic Regression Model [weighted] with default parameters) \\033[0m\")\n",
    "determine_feature_importance_lr(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.2 Tree-Based Models\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/dt.png?raw=true\" width=\"15%\" >\n",
    "\n",
    "We explored the following classification algorithms:  decision trees, random forests, and gradient-boosted trees. For each of the models, we first explored performance with the complete, unbalanced dataset. Next we explored the models with a balanced dataset created from undersampling the majority class. Finally, we implemented bootstrap aggregation (bagging), which we applied in conjunction with a balanced dataset (in terms of majority and minority classes) on all three algorithms.\n",
    "\n",
    "We performed hyperparameter tuning with gridsearch using crossfold validation, but have omitted that here because these baseline models performed so poorly that no amount of hyperparameter tuning made a meaningful difference in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1 Baseline Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.1 Decision Tree - Baseline\n",
    "\n",
    "Decision trees can be advantageous for several reasons. They handle numeric, categorical, and ordinal features without requiring pre-processing (standardization or scaling). Further, decision trees handle missing values naturally, which was a challenge for our dataset, especially the weather data. They are also explainable, as we can inspect the tree structure and infer the relative importance of features based on where they are in the tree hierarchy. (The display of the tree is quite large, so we show the code but do not run it here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Decision Tree Model (Baseline) </span>\n",
       "\n",
       "Trained model in 0.7926605939865112 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t58,997\n",
       "tn (correct   pred as ontime)\t2,631,727\n",
       "fp (incorrect pred as delay) \t38,903\n",
       "fn (incorrect pred as ontime)\t614,138, &#34;\n",
       "&#34;\n",
       "accuracy \t0.8047 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.6026 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.0876 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.153\n",
       "Area under PR = 0.07604838330527905\n",
       "\n",
       "Evaluated metrics in 0.32008949518203733 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify the decision tree classifier model and fit to the training data\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 10)\n",
    "start = time.time()\n",
    "dtModel = dt.fit(data_train_tree)\n",
    "print(\"\\033[1m Decision Tree Model (Baseline) \\033[0m\")\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = dtModel.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"DecisionTree_Baseline\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(dtModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.2 Random Forest - Baseline\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/rf.png?raw=true\" width=\"15%\" >\n",
    "\n",
    "Random forests are built from a number of trees that are built on sub-samples of the features (columns). By only taking a subsection of the possible features, the individual trees are forced to consider different combinations of features. For classification, the mode prediction is returned. By creating a number of individual trees and returning the mode, random forests correct for the tendency of decision trees to overfit (James, Witten, Hastie, Tibshirani, 2017). We performed gridsearch and crossfold validation on these models (ommited here), but the results did not improve very much from those shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Random Forest (Baseline) </span>\n",
       "\n",
       "Trained model in 26.360087764263152 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t49,395\n",
       "tn (correct   pred as ontime)\t2,650,421\n",
       "fp (incorrect pred as delay) \t20,209\n",
       "fn (incorrect pred as ontime)\t623,740, &#34;\n",
       "&#34;\n",
       "accuracy \t0.8074 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.7097 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.0734 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.133\n",
       "Area under PR = 0.06574967550011826\n",
       "\n",
       "Evaluated metrics in 1.91375466187795 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create random forest model and train on the training set\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",  \n",
    "                                     numTrees=50, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', subsamplingRate=0.8, maxDepth=15, maxBins=32)\n",
    "start = time.time()\n",
    "rfModel = rf.fit(data_train_tree)\n",
    "print(\"\\033[1m Random Forest (Baseline) \\033[0m\")\n",
    "\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = rfModel.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"RandomForest_Baseline\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.3 Gradient-Boosted Trees - Baseline\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/gbt.png?raw=true\" width=\"10%\" >\n",
    "\n",
    "Gradient-boosted trees are a subset of the class of gradient-boosted models that use decision trees as base learners. These base learner trees are are weak models that learn direction vectors with both direction and magnitude (Parr and Howard, 2020). Subsequent learners build off previous ones (much like a good golfer hits progressively shorter shots as she moves from tee to green to the hole). We experimented with tuning the number of iterations, but observed little difference in model performance when the full training set was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Gradient Boosted Trees (Baseline) </span>\n",
       "\n",
       "Trained model in 6.6654411315917965 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t44,536\n",
       "tn (correct   pred as ontime)\t2,645,870\n",
       "fp (incorrect pred as delay) \t24,760\n",
       "fn (incorrect pred as ontime)\t628,599, &#34;\n",
       "&#34;\n",
       "accuracy \t0.8046 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.6427 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.0662 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.12\n",
       "Area under PR = 0.058044365996787875\n",
       "\n",
       "Evaluated metrics in 0.10768393278121949 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create gradient-boosted tree model and train on the training set\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxIter=20)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "gbtModel = gbt.fit(data_train_tree)\n",
    "print(\"\\033[1m Gradient Boosted Trees (Baseline) \\033[0m\")\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = gbtModel.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"GBT_Baseline\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.2 Tree-Based Models Using a Balanced Dataset\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/bagging.png?raw=true\" width=\"40%\" >\n",
    "\n",
    "Next, we attempted to improve the model performance by creating a balanced dataset by randomly undersampling the majority class. This had the effect of reducing our training set from ~24M records to ~9M records. We explored each of the three models again and experimented further with hyperparameter tuning, both manually and through gridsearch and crossfold validation (this was a very computationally expensive operation, especially for forests and gradient-boosted trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper functions to create data sets balanced on the outcome variable\n",
    "\n",
    "def split_data(data):\n",
    "  \"\"\"\n",
    "  Function that takes in dataframe and splits it into two based on the outcome variable and returns two dataframes\n",
    "  \"\"\"\n",
    "  data_ontime = data.filter('OUTCOME = 0')\n",
    "  data_delayed = data.filter('OUTCOME = 1')\n",
    "  return data_ontime, data_delayed\n",
    "\n",
    "def create_balanced_set(data):\n",
    "  \"\"\"\n",
    "  This function takes in a dataframe, splits the dataframe by its outcome variable, calculates a balancing ratio and\n",
    "  creates a new dataset balanced by the outcome variable by under-sampling the majority class.\n",
    "  \"\"\"\n",
    "  on_time, delayed = split_data(data)\n",
    "  on_time_count = on_time.count()\n",
    "  delayed_count = delayed.count()\n",
    "  ratio = delayed_count / on_time_count\n",
    "      \n",
    "  #take a random sample from the data\n",
    "  current_seed = np.random.randint(low=1, high=100, size=1)[0]\n",
    "  on_time_samp = on_time.sample(False, ratio, seed=int(current_seed))\n",
    "  delayed_samp = delayed.sample(True, 1.0, seed=int(current_seed))\n",
    "      \n",
    "  #combine into single dataframe\n",
    "  balanced_train = on_time_samp.union(delayed_samp).cache()\n",
    "  return balanced_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[22]: 9353751</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the balanced training set based on the tree pipeline\n",
    "balanced_training = create_balanced_set(data_train_tree)\n",
    "balanced_training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.1 Decision Tree - Balanced Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Decision Tree Model (Balanced Outcome) </span>\n",
       "\n",
       "Trained model in 0.2418828288714091 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t411,813\n",
       "tn (correct   pred as ontime)\t1,780,552\n",
       "fp (incorrect pred as delay) \t890,078\n",
       "fn (incorrect pred as ontime)\t261,322, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6557 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3163 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6118 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.417\n",
       "Area under PR = 0.5357464453795633\n",
       "\n",
       "Evaluated metrics in 0.09859071175257365 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Specify the decision tree classifier model and fit to the training data\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 10)\n",
    "start = time.time()\n",
    "dtModel = dt.fit(balanced_training)\n",
    "print(\"\\033[1m Decision Tree Model (Balanced Outcome) \\033[0m\")\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = dtModel.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"Decision_Tree_Balanced\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>0.270109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>ORIGIN_PRECIP_RATE_AVG</td>\n",
       "      <td>0.149896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>DEST_PRECIP_RATE_AVG</td>\n",
       "      <td>0.129083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>ORIGIN_VIS_AVG</td>\n",
       "      <td>0.080427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>OP_UNIQUE_CARRIER_ORDINAL</td>\n",
       "      <td>0.069378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>ORIGIN_TMP_AVG</td>\n",
       "      <td>0.056768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MONTH</td>\n",
       "      <td>0.039007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>DEST_ELEV</td>\n",
       "      <td>0.028351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>DEST_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.022225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ORIGIN_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>ORIGIN_LONGITUDE</td>\n",
       "      <td>0.019898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>DEST_VIS_AVG</td>\n",
       "      <td>0.011527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>DEST_TMP_AVG</td>\n",
       "      <td>0.011204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DEST_ORDINAL</td>\n",
       "      <td>0.010351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>0.009416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>0.008584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>ORIGIN_LATITUDE</td>\n",
       "      <td>0.005817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>DEST_DEW_AVG</td>\n",
       "      <td>0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>DEST_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>DEST_LONGITUDE</td>\n",
       "      <td>0.005497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ORIGIN_ORDINAL</td>\n",
       "      <td>0.005478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ORIGIN_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.005344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>DEST_CIG_AVG</td>\n",
       "      <td>0.005254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>ORIGIN_DEW_AVG</td>\n",
       "      <td>0.004321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>ORIGIN_CIG_AVG</td>\n",
       "      <td>0.002753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>DEST_WND_SPEED_AVG</td>\n",
       "      <td>0.002641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>ORIGIN_WND_SPEED_AVG</td>\n",
       "      <td>0.001866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>DEST_PAGERANK</td>\n",
       "      <td>0.001442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ORIGIN_ELEV</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>ORIGIN_WND_ANGLE_AVG</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>ORIGIN_SLP_AVG</td>\n",
       "      <td>0.001077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>DEST_WND_ANGLE_AVG</td>\n",
       "      <td>0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>0.000874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>DEST_SLP_AVG</td>\n",
       "      <td>0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>DEST_LATITUDE</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>ORIGIN_PAGERANK</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>ORIGIN_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>QUARTER</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>DEST_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display feature imporance\n",
    "print(\"\\033[1mFeature Importance : Decision Tree Model [weighted]) \\033[0m\")\n",
    "display_feature_importance_tree(dtModel, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the decision tree (output omitted due to large size)\n",
    "#display(dtModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.2 Random Forest - Balanced Outcomes\n",
    "\n",
    "The Random Forest model trained on balanced training data uses hyperparameters that were based on the results of the gridsearch crossfold validation performed on the small sample set of data. We displayed the feature importance so that we could compare how the different models treated features differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Random Forest Model (Balanced Outcome) </span>\n",
       "<span class=\"ansi-bold\"> hyperparameters tuned with smaller gridsearch </span>\n",
       "\n",
       "Trained model in 44.96609122355779 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t387,985\n",
       "tn (correct   pred as ontime)\t1,970,553\n",
       "fp (incorrect pred as delay) \t700,077\n",
       "fn (incorrect pred as ontime)\t285,150, &#34;\n",
       "&#34;\n",
       "accuracy \t0.7054 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3566 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.5764 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4406\n",
       "Area under PR = 0.4956412271968361\n",
       "\n",
       "Evaluated metrics in 9.243221839269003 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pull out the parameters from the gridsearch and re-run the RF model on the full training set to re-train the weights\n",
    "best_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",  \n",
    "                                     numTrees=25, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=20, maxBins=32)\n",
    "\n",
    "start = time.time()\n",
    "rfModel_bal = best_rf.fit(balanced_training)\n",
    "print(\"\\033[1m Random Forest Model (Balanced Outcome) \\033[0m\")\n",
    "print(\"\\033[1m hyperparameters tuned with smaller gridsearch \\033[0m\")\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = rfModel_bal.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions,\"Random_Forest_Balanced\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>0.092413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>0.059856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>DEST_PRECIP_RATE_AVG</td>\n",
       "      <td>0.051311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>ORIGIN_PRECIP_RATE_AVG</td>\n",
       "      <td>0.049372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>ORIGIN_VIS_AVG</td>\n",
       "      <td>0.046293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>ORIGIN_TMP_AVG</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>DEST_VIS_AVG</td>\n",
       "      <td>0.032334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>OP_UNIQUE_CARRIER_ORDINAL</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>DEST_CIG_AVG</td>\n",
       "      <td>0.029228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>ORIGIN_DEW_AVG</td>\n",
       "      <td>0.028917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>ORIGIN_CIG_AVG</td>\n",
       "      <td>0.028393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>DEST_DEW_AVG</td>\n",
       "      <td>0.025846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>ORIGIN_WND_SPEED_AVG</td>\n",
       "      <td>0.025494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>DEST_WND_SPEED_AVG</td>\n",
       "      <td>0.025426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>0.024934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>DEST_TMP_AVG</td>\n",
       "      <td>0.024553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>ORIGIN_SLP_AVG</td>\n",
       "      <td>0.024213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MONTH</td>\n",
       "      <td>0.022812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>DEST_SLP_AVG</td>\n",
       "      <td>0.022522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>DEST_WND_ANGLE_AVG</td>\n",
       "      <td>0.022465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>ORIGIN_WND_ANGLE_AVG</td>\n",
       "      <td>0.022291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>0.018482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>DEST_LONGITUDE</td>\n",
       "      <td>0.018168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>ORIGIN_LONGITUDE</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>DEST_ELEV</td>\n",
       "      <td>0.016905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ORIGIN_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.016376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>0.015015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>0.014838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DEST_ORDINAL</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>ORIGIN_PAGERANK</td>\n",
       "      <td>0.014397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>DEST_PAGERANK</td>\n",
       "      <td>0.014366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>ORIGIN_LATITUDE</td>\n",
       "      <td>0.014060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>DEST_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.013606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ORIGIN_ELEV</td>\n",
       "      <td>0.013470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ORIGIN_ORDINAL</td>\n",
       "      <td>0.013091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>DEST_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.012657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>0.012551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>DEST_LATITUDE</td>\n",
       "      <td>0.012249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ORIGIN_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.011360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>QUARTER</td>\n",
       "      <td>0.005723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>ORIGIN_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>DEST_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display feature importance\n",
    "print(\"\\033[1mFeature Importance : Random Forest Model [weighted]) \\033[0m\")\n",
    "display_feature_importance_tree(rfModel_bal, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.1.3 Gradient-Boosted Trees - Balanced Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Gradient Boosted Tree Model (Balanced Outcome) </span>\n",
       "\n",
       "Trained model in 2.0854694485664367 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t419,687\n",
       "tn (correct   pred as ontime)\t1,783,909\n",
       "fp (incorrect pred as delay) \t886,721\n",
       "fn (incorrect pred as ontime)\t253,448, &#34;\n",
       "&#34;\n",
       "accuracy \t0.659 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3213 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6235 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.424\n",
       "Area under PR = 0.5444812685773592\n",
       "\n",
       "Evaluated metrics in 0.10978469053904215 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create GBT model and train on the balanced training set\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxIter=20)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "gbtModel_bal = gbt.fit(balanced_training)\n",
    "print(\"\\033[1m Gradient Boosted Tree Model (Balanced Outcome) \\033[0m\")\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "# Generate predictions on the dev set and evaluate model performance\n",
    "predictions = gbtModel_bal.transform(data_dev_tree).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions,\"GBT_Balanced\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>OP_UNIQUE_CARRIER_ORDINAL</td>\n",
       "      <td>0.093186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>0.063047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>ORIGIN_TMP_AVG</td>\n",
       "      <td>0.051811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>DEST_VIS_AVG</td>\n",
       "      <td>0.045909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>DEST_TMP_AVG</td>\n",
       "      <td>0.044994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>0.043897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>DEST_DEW_AVG</td>\n",
       "      <td>0.039952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>0.037668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>ORIGIN_PRECIP_RATE_AVG</td>\n",
       "      <td>0.035862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>ORIGIN_LONGITUDE</td>\n",
       "      <td>0.035392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>DEST_WND_SPEED_AVG</td>\n",
       "      <td>0.032871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>DEST_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.032751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>DEST_ELEV</td>\n",
       "      <td>0.031559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>ORIGIN_VIS_AVG</td>\n",
       "      <td>0.029557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>DEST_LONGITUDE</td>\n",
       "      <td>0.029302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>ORIGIN_DEW_AVG</td>\n",
       "      <td>0.029263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ORIGIN_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.028182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>ORIGIN_SLP_AVG</td>\n",
       "      <td>0.027582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>ORIGIN_WND_SPEED_AVG</td>\n",
       "      <td>0.025886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MONTH</td>\n",
       "      <td>0.025855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>DEST_CIG_AVG</td>\n",
       "      <td>0.023808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>DEST_PRECIP_RATE_AVG</td>\n",
       "      <td>0.021564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ORIGIN_STATE_FIPS_ORDINAL</td>\n",
       "      <td>0.020346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ORIGIN_ORDINAL</td>\n",
       "      <td>0.017295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ORIGIN_ELEV</td>\n",
       "      <td>0.016249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>DEST_PAGERANK</td>\n",
       "      <td>0.015146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DEST_ORDINAL</td>\n",
       "      <td>0.014457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>ORIGIN_LATITUDE</td>\n",
       "      <td>0.013176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>DEST_CITY_MARKET_ID_ORDINAL</td>\n",
       "      <td>0.012651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>DEST_LATITUDE</td>\n",
       "      <td>0.012224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>ORIGIN_CIG_AVG</td>\n",
       "      <td>0.012150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>0.007101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>DEST_SLP_AVG</td>\n",
       "      <td>0.006744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>ORIGIN_PAGERANK</td>\n",
       "      <td>0.006579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>DEST_WND_ANGLE_AVG</td>\n",
       "      <td>0.004285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>ORIGIN_WND_ANGLE_AVG</td>\n",
       "      <td>0.003608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>0.001849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>DEST_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>ORIGIN_SNOW_DEPTH_AVG</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>YEAR</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>QUARTER</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display feature importance\n",
    "print(\"\\033[1mFeature Importance : Gradient Boosted Tree Model [weighted]) \\033[0m\")\n",
    "display_feature_importance_tree(gbtModel_bal, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.3 Bootstrap Aggregation (Bagging)\n",
    "\n",
    "Bagging implements a logic similar to that of random forests, but it operates on the rows (data observations) instead of the columns (features). It's underlying principle is to build complex, high variance trees (that individually might be overfit) and then overcome the high variance by averaging. We developed the bagging framework as a way to continue to train on balanced training sets, but be able to take advantage of all of our training data. We implemented bagging for decision trees, random forests, and gradient boosted trees. These models were expensive to train, so we implemented a human-based gridsearch whereby we split the different parameter settings we wanted to run between our four team members and had us each train different models. Depending on the performance of the databricks cluster at runtime, these models took up to ten hours to train. (We incorporated interim print statements to help us keep track of model training progress.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a set of helper functions that are used in running the bootstrap aggregation (bagging) on the tree-based models (decision tree, random forest, gradient-boosted trees)\n",
    "\n",
    "\n",
    "def eval_tree(the_data_train, the_data_eval, nDepth):\n",
    "  \"\"\"\n",
    "  Function that takes in a trainig dataframe, a test dataframe, and a depth hyperparameter, \n",
    "  then trains a decision tree on it and returns\n",
    "  a new dataframe with columns 'id' and 'prediction'\n",
    "  \"\"\"\n",
    "  predictions = dt = dtModel = tree_eval = None\n",
    "  \n",
    "  # Define the decision tree model, fit on the training data, generate and cache predictions on the eval data\n",
    "  dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = nDepth)\n",
    "  dtModel = dt.fit(the_data_train)\n",
    "  predictions = dtModel.transform(the_data_eval).cache()\n",
    "\n",
    "  #Create new 'predictions' and 'one_prediction' df each time through\n",
    "  #Create new dataframe taht only captures the row 'id' and prediction\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "  \n",
    "  one_prediction = predictions.select('id', 'prediction')\n",
    "  one_prediction.createOrReplaceTempView(\"one_prediction\")\n",
    "\n",
    "  return one_prediction\n",
    "\n",
    "\n",
    "def eval_forest(the_data_train, the_data_eval, nDepth):\n",
    "  \"\"\"\n",
    "  Function that takes in a trainig dataframe, a test dataframe, and a depth hyperparameter, \n",
    "  then trains a random forest on it and returns\n",
    "  a new dataframe with columns 'id' and 'prediction'\n",
    "  \"\"\"\n",
    "  predictions = dt = dtModel = tree_eval = None\n",
    "  nTrees = 25\n",
    "  \n",
    "  # Define the random forest model, fit on the training data, generate and cache predictions on the eval data\n",
    "  rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",  \n",
    "                                     numTrees=nTrees, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', subsamplingRate=0.8, maxDepth=nDepth, maxBins=32)\n",
    "  rfModel = rf.fit(the_data_train)\n",
    "  predictions = rfModel.transform(the_data_eval).cache()\n",
    "\n",
    "  #Create new 'predictions' and 'one_prediction' df each time through\n",
    "  #Create new dataframe taht only captures the row 'id' and prediction\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "  \n",
    "  one_prediction = predictions.select('id', 'prediction')\n",
    "  one_prediction.createOrReplaceTempView(\"one_prediction\")\n",
    "\n",
    "  return one_prediction\n",
    "\n",
    "\n",
    "def eval_gbt(the_data_train, the_data_eval):\n",
    "  \"\"\"\n",
    "  Function that takes in a trainig dataframe, a test dataframe, and a depth hyperparameter, \n",
    "  then trains a gradient-boosted tree model on it and returns\n",
    "  a new dataframe with columns 'id' and 'prediction'\n",
    "  \"\"\"\n",
    "  predictions = dt = dtModel = tree_eval = None\n",
    "    \n",
    "  # Define the GBT model, fit on the training data, generate and cache predictions on the eval data\n",
    "  gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",maxIter=20)\n",
    "  \n",
    "  gbtModel = gbt.fit(the_data_train)\n",
    "  predictions = gbtModel.transform(the_data_eval).cache()\n",
    "\n",
    "  #Create new 'predictions' and 'one_prediction' df each time through\n",
    "  #Create new dataframe taht only captures the row 'id' and prediction\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "  \n",
    "  one_prediction = predictions.select('id', 'prediction')\n",
    "  one_prediction.createOrReplaceTempView(\"one_prediction\")\n",
    "\n",
    "\n",
    "  return one_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There is a slight modification to the evaluate performance function to make it work more cleanly when called multiple times.\n",
    "\n",
    "def evaluateTreePerformance(modelName, predictions):\n",
    "  \"\"\"\n",
    "  Function that takes in a dataframe that includes a column of 'labels' and 'predictions' and\n",
    "  parses these labels and predictions to calculate, print, and return as a list the suite of accuracy measures\n",
    "  we are interested in.\n",
    "  \"\"\"\n",
    "  \n",
    "  # manual counts\n",
    "  tp = spark.sql(\"select count(*) from predictions where label = 1 and prediction = 1\").first()[0]\n",
    "  tn = spark.sql(\"select count(*) from predictions where label = 0 and prediction = 0\").first()[0]\n",
    "  fp = spark.sql(\"select count(*) from predictions where label = 0 and prediction = 1\").first()[0]\n",
    "  fn = spark.sql(\"select count(*) from predictions where label = 1 and prediction = 0\").first()[0]\n",
    "  \n",
    "\n",
    "  # Overall statistics\n",
    "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "  recall = tp / (tp+fn)\n",
    "  \n",
    "  precision = 0\n",
    "  if tp+fp != 0:\n",
    "    precision = tp / (tp+fp)\n",
    "\n",
    "  f1score = 0\n",
    "  if recall + precision != 0:\n",
    "    f1score = 2*recall*precision/(recall + precision)\n",
    "\n",
    "  \n",
    "  # Area under precision-recall curve\n",
    "  pl_rdd = predictions.select(predictions.label.cast(\"float\"), predictions.prediction.cast(\"float\")).rdd\n",
    "  pr_metrics = BinaryClassificationMetrics(pl_rdd)\n",
    "  auc = pr_metrics.areaUnderPR\n",
    "\n",
    "  print(\"Summary Stats\")\n",
    "  print(\"tp (correct   pred as delay) \\t%s\" % f'{tp:,}')\n",
    "  print(\"tn (correct   pred as ontime)\\t%s\" % f'{tn:,}')\n",
    "  print(\"fp (incorrect pred as delay) \\t%s\" % f'{fp:,}')\n",
    "  print(\"fn (incorrect pred as ontime)\\t%s\" % f'{fn:,}, \"\\n\"')\n",
    "\n",
    "  print(\"accuracy \\t%s\"  % np.round(accuracy,4),  \"\\tout of all observations, how many were predicted correctly?\")\n",
    "  print(\"precision\\t%s\"  % np.round(precision,4), \"\\tout of all delays predicted, how many are correct?\")\n",
    "  print(\"recall   \\t%s\"  % np.round(recall,4),    \"\\tout of all actual delays, how many are correctly predicted?\")\n",
    "  print(\"f1       \\t%s\"  % np.round(f1score,4))\n",
    "  print(\"Area under PR = %s\" % auc)\n",
    "  \n",
    "  #Store the metrics in a list to return\n",
    "  tree_metrics = [accuracy,precision,recall,f1score, auc]\n",
    "  \n",
    "  #Store the metrics in a list to return\n",
    "  predictionDF = pd.read_csv(metrics_filename, index_col=0)\n",
    "  predictionDF.loc[modelName] = [accuracy, precision, recall, f1score, auc, tp, tn, fp, fn]\n",
    "  predictionDF.to_csv(metrics_filename)\n",
    "  \n",
    "  #reset the predictions for the next iteration\n",
    "  predictions = tp = tn = fp = fn = None\n",
    "  return tree_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bagging(nTrees, nDepth, bagFrac, data_train, data_eval, model_type):\n",
    "  \"\"\"\n",
    "  Function that performs bagging on a balanced dataset for a specified number of trees/random forests / GBTs.\n",
    "  'nTrees' is an integer that specifies the number of bagged trees to train\n",
    "  'nDepth' is an integer that specifies the depth for trees/forests\n",
    "  'data_train' is a dataframe of training data\n",
    "  'data_eval' is a dataframe of evaluation data\n",
    "  'model_type' is a string that accepts 'tree', 'forest', or 'gbt' and calls the specified model for each bagged tree\n",
    "  \n",
    "  Function returns the performance metrics for the bagged models.\n",
    "  \"\"\"\n",
    "  #create new version of data_train\n",
    "  data_train.createOrReplaceTempView(\"data_train\")\n",
    "  \n",
    "  #split training data into on-time and delayed, generate counts and create a ratio\n",
    "  on_time, delayed = split_data(data_train)\n",
    "  on_time_count = on_time.count()\n",
    "  delayed_count = delayed.count()\n",
    "  ratio = delayed_count / on_time_count\n",
    "\n",
    "  ## create dataframe to hold the predictions\n",
    "  data_eval.createOrReplaceTempView(\"data_eval\")\n",
    "  prediction_df = data_eval.select('id', 'label')\n",
    "    \n",
    "\n",
    "  #loop to generate individual trees in ensemble\n",
    "  for i in range(nTrees):\n",
    "    start = time.time()\n",
    "        \n",
    "    #take a random sample from the data for each tree\n",
    "    current_seed = np.random.randint(low=1, high=100, size=1)[0]\n",
    "    on_time_samp = on_time.sample(False, bagFrac*ratio, seed=int(current_seed))\n",
    "    delayed_samp = delayed.sample(True, bagFrac, seed=int(current_seed))\n",
    "      \n",
    "    #combine into single dataframe, replace old version in memory and cache\n",
    "    balanced_train = on_time_samp.union(delayed_samp).cache()\n",
    "    balanced_train.createOrReplaceTempView(\"balanced_train\")   \n",
    "        \n",
    "    #build a tree on the sampled data\n",
    "    if model_type == 'tree':\n",
    "      one_prediction = eval_tree(balanced_train, data_eval, nDepth)\n",
    "    elif model_type == 'forest':\n",
    "      one_prediction = eval_forest(balanced_train, data_eval, nDepth)\n",
    "    elif model_type == 'gbt':\n",
    "      one_prediction = eval_gbt(balanced_train, data_eval)\n",
    "    else:\n",
    "      print('Do not recognize model type')\n",
    "      break\n",
    "    \n",
    "    #join predictions from latest tree to dataframe of predictions\n",
    "    prediction_df = prediction_df.join(one_prediction, 'id','left')\n",
    "    prediction_df = prediction_df.withColumnRenamed('prediction',f'prediction_{i}')\n",
    "\n",
    "\n",
    "      \n",
    "    # Clear the sample and metrics variables for next tree\n",
    "    on_time_samp = delayed_samp = tree_metrics = None\n",
    "    print('Completed bag ', i)\n",
    "    print(f\"\\nCompleted bag in {(time.time() - start)/60} minutes\")\n",
    "  \n",
    "  #once all bagged predictions complete, calculate majority vote prediction\n",
    "  n = lit(len(prediction_df.columns) - 2.0)\n",
    "  rowMean  = (reduce(add, (col(x) for x in prediction_df.columns[2:])) / n)\n",
    "  prediction_df = prediction_df.withColumn(\"prediction_mean\", rowMean)\n",
    "  rowVote = when(col(\"prediction_mean\") >= 0.5, 1).otherwise(0)\n",
    "  prediction_df = prediction_df.withColumn(\"prediction\", rowVote)\n",
    "  prediction_df.createOrReplaceTempView(\"prediction_df\")\n",
    "  LabelsandPredictions    = spark.sql(\"select label, prediction from prediction_df\")\n",
    "    \n",
    "  #return evaluation metrics \n",
    "  modelDisplayName = model_type + \"_\" + str(nTrees) + \"bags_\" + str(nDepth) + \"deep\"\n",
    "  metrics = evaluateTreePerformance(modelDisplayName, LabelsandPredictions)\n",
    "\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.3.1 Decision Tree - Bootstrap Aggregation (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Bagged Tree Ensemble </span>\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Number of trees:  12\n",
       "Tree depth:  12\n",
       "Completed bag  0\n",
       "\n",
       "Completed bag in 0.6415027936299642 minutes\n",
       "Completed bag  1\n",
       "\n",
       "Completed bag in 0.6665144960085551 minutes\n",
       "Completed bag  2\n",
       "\n",
       "Completed bag in 0.592058519522349 minutes\n",
       "Completed bag  3\n",
       "\n",
       "Completed bag in 0.6923970381418864 minutes\n",
       "Completed bag  4\n",
       "\n",
       "Completed bag in 0.5499250491460165 minutes\n",
       "Completed bag  5\n",
       "\n",
       "Completed bag in 0.5092409968376159 minutes\n",
       "Completed bag  6\n",
       "\n",
       "Completed bag in 0.6764811515808106 minutes\n",
       "Completed bag  7\n",
       "\n",
       "Completed bag in 0.5359977165857951 minutes\n",
       "Completed bag  8\n",
       "\n",
       "Completed bag in 0.34807341893514 minutes\n",
       "Completed bag  9\n",
       "\n",
       "Completed bag in 0.3990460395812988 minutes\n",
       "Completed bag  10\n",
       "\n",
       "Completed bag in 0.6850247661272685 minutes\n",
       "Completed bag  11\n",
       "\n",
       "Completed bag in 0.49212029774983723 minutes\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t402,937\n",
       "tn (correct   pred as ontime)\t1,823,734\n",
       "fp (incorrect pred as delay) \t846,896\n",
       "fn (incorrect pred as ontime)\t270,198, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6659 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3224 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.5986 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4191\n",
       "Area under PR = 0.5413769148323084\n",
       "\n",
       "Trained Model in 7.8899408658345545 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save hyperparameters that are fed into the bagging model\n",
    "nTrees = 12\n",
    "nDepth = 12\n",
    "bagFrac = 0.9\n",
    "\n",
    "# Run the bagging algorithm, including generating predictions and evaluating model performance\n",
    "print(\"\\033[1m Bagged Tree Ensemble \\033[0m\")\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "print('Number of trees: ', nTrees)\n",
    "print('Tree depth: ', nDepth)\n",
    "start = time.time()\n",
    "bagging(nTrees, nDepth, bagFrac, data_train_tree, data_dev_tree, 'tree')\n",
    "print(f\"\\nTrained Model in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.3.2 Random Forest - Bootstrap Aggregation (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Bagged Random Forest Ensemble </span>\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Number of trees:  12\n",
       "Tree depth:  12\n",
       "Completed bag  0\n",
       "\n",
       "Completed bag in 2.307594132423401 minutes\n",
       "Completed bag  1\n",
       "\n",
       "Completed bag in 2.275724768638611 minutes\n",
       "Completed bag  2\n",
       "\n",
       "Completed bag in 2.2220126231511435 minutes\n",
       "Completed bag  3\n",
       "\n",
       "Completed bag in 2.084809203942617 minutes\n",
       "Completed bag  4\n",
       "\n",
       "Completed bag in 1.8929582277933756 minutes\n",
       "Completed bag  5\n",
       "\n",
       "Completed bag in 2.096487792332967 minutes\n",
       "Completed bag  6\n",
       "\n",
       "Completed bag in 2.3871935923894245 minutes\n",
       "Completed bag  7\n",
       "\n",
       "Completed bag in 2.1775803049405416 minutes\n",
       "Completed bag  8\n",
       "\n",
       "Completed bag in 2.1798290332158405 minutes\n",
       "Completed bag  9\n",
       "\n",
       "Completed bag in 1.973476481437683 minutes\n",
       "Completed bag  10\n",
       "\n",
       "Completed bag in 2.174145249525706 minutes\n",
       "Completed bag  11\n",
       "\n",
       "Completed bag in 2.0450844208399457 minutes\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t422,136\n",
       "tn (correct   pred as ontime)\t1,814,276\n",
       "fp (incorrect pred as delay) \t856,354\n",
       "fn (incorrect pred as ontime)\t250,999, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6688 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3302 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6271 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4326\n",
       "Area under PR = 0.5502170337211968\n",
       "\n",
       "Trained Model in 28.14997968276342 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Establish hyperparameters\n",
    "nTrees = 12\n",
    "nDepth = 12\n",
    "bagFrac = 0.9\n",
    "\n",
    "# Run the bagging algorithm, generate predictions and evaluate model performance\n",
    "print(\"\\033[1m Bagged Random Forest Ensemble \\033[0m\")\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "print('Number of trees: ', nTrees)\n",
    "print('Tree depth: ', nDepth)\n",
    "start = time.time()\n",
    "bagging(nTrees, nDepth, bagFrac, data_train_tree, data_dev_tree, 'forest')\n",
    "print(f\"\\nTrained Model in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4.2.3.3 Gradient-Boosted Tree - Bootstrap Aggregation (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Bagged Gradient Boosted Tree Ensemble </span>\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Number of trees:  12\n",
       "Tree depth:  12\n",
       "Completed bag  0\n",
       "\n",
       "Completed bag in 1.606032141049703 minutes\n",
       "Completed bag  1\n",
       "\n",
       "Completed bag in 1.4227243582407634 minutes\n",
       "Completed bag  2\n",
       "\n",
       "Completed bag in 1.4466995159784952 minutes\n",
       "Completed bag  3\n",
       "\n",
       "Completed bag in 1.2884631077448527 minutes\n",
       "Completed bag  4\n",
       "\n",
       "Completed bag in 1.2971604903539022 minutes\n",
       "Completed bag  5\n",
       "\n",
       "Completed bag in 1.4524115641911826 minutes\n",
       "Completed bag  6\n",
       "\n",
       "Completed bag in 1.915587031841278 minutes\n",
       "Completed bag  7\n",
       "\n",
       "Completed bag in 1.7247920950253806 minutes\n",
       "Completed bag  8\n",
       "\n",
       "Completed bag in 1.6051070014635722 minutes\n",
       "Completed bag  9\n",
       "\n",
       "Completed bag in 1.408640968799591 minutes\n",
       "Completed bag  10\n",
       "\n",
       "Completed bag in 1.594395967324575 minutes\n",
       "Completed bag  11\n",
       "\n",
       "Completed bag in 1.3635121623675028 minutes\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t419,136\n",
       "tn (correct   pred as ontime)\t1,785,933\n",
       "fp (incorrect pred as delay) \t884,697\n",
       "fn (incorrect pred as ontime)\t253,999, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6595 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3215 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6227 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.424\n",
       "Area under PR = 0.5482360873774347\n",
       "\n",
       "Trained Model in 19.58750693400701 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "nTrees = 12\n",
    "nDepth = 12\n",
    "bagFrac = 0.9\n",
    "\n",
    "# Run bagging algorithm, generate predictions and evaluate model performance\n",
    "print(\"\\033[1m Bagged Gradient Boosted Tree Ensemble \\033[0m\")\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "print('Number of trees: ', nTrees)\n",
    "print('Tree depth: ', nDepth)\n",
    "start = time.time()\n",
    "bagging(nTrees, nDepth, bagFrac, data_train_tree, data_dev_tree, 'gbt')\n",
    "print(f\"\\nTrained Model in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3 Feature Significance review\n",
    "\n",
    "In each of our model types above, we extracted feature significance metrics and we summarize these in the table below. This was a helpful exercise to help us build intuition into which were the most relevant features - and in particular which features were consistently relevant across different algorithm types. We see that departure time, month of flight, and airline carrier code are consistently important features, whereas other features expected to be consistently important such as rainfall and visibility, are not consistently top ranking features across these algorithm types. \n",
    "\n",
    "As we examined feature importance for the different models, it was interesting to note how the ordered list of features changed from model to model. As an example, the departure time of day was the most influential feature for decison tree, random forest, and gradient boosted tree examples shown, but the fifth most important for logistic regression. Similarly, the rate of precipitation at the origin ranged from 18th (gradient boosted tree) to second (decision tree). These results were puzzling given how similarly the different models performed.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/leebean337/w261_images/blob/master/feature_importance.png?raw=true\" width=\"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4 Final Algorithm: Logistic Regression Theory and Toy Example\n",
    "\n",
    "Despite considerable effort dedicated to our tree-based modelling approaches, we ultimately selected logistic regression for our final algorithm given that it provided the best performance, both in term of f1-score and recall as well as runtime. In our case, there was little trade-off required in this decision. This section provides more background on the theory behind logistic regression, the gradient descent algorithm by which it is applied and a toy example outlining the steps involved in one iteration of gradient descent.\n",
    "\n",
    "Logistic regression is similar to linear regression in as far as it begins by fitting a linear relationship between the features and the outcome variable by determining the optimal weights associated with each feature. The key difference is that the outcome variable is binary and the resulting prediction from a logistic regression model must therefore be a binary classification as well. In terms of interpretation, a unit change in a feature will multiply the odds of the outcome variables by a constant factor (the associated weight). This technique relies on the use of a sigmoid function (g(z)) which translates the raw output of the linear regression (z) into a probability value between 0 and 1. This results in the predicted probability (h(x)) that a particular example belongs to a specific class (usually the positive class).\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/raw/master/LR%20sigmoid.jpg?raw=true\"/>\n",
    "\n",
    "Using the toy example below, we will demonstrate how our algorithm will apply logistic regression using standard gradient descent. Our goal is to find the optimal parameters (), which will minimize our logistic loss function J() where m is the number of training examples and for each training example, the true value of y determines whether the log of the predicted probability (log(h(x))) or log of one minus the predicted probability (log(1-(h(x))) will factor into the total loss calculation.\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/raw/master/LR%20cost%20function.jpg?raw=true\"/>\n",
    "\n",
    "Gradient descent is an iterative optimisation technique to find the minimum of the loss function, by taking steps down the gradient of the function until a set of parameters () which are sufficiently close to the minimum of the loss are identified. To determine the gradient step, we take the derivative of the loss function \\\\( \\nabla\\_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\\\)\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/raw/master/LR%20gradient%20calculation.jpg?raw=true\"/>\n",
    "\n",
    "This array is then multiplied by the learning rate and then subtracted from the prior set of parameters () and the process is repeated again. This is continued until _old and _new are sufficiently close to one another that it doesnt make sense to continue updating. \n",
    "\n",
    "\\\\(  \\theta\\_{\\text{new}} = \\theta\\_{\\text{old}} - \\eta \\cdot \\nabla\\_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\\\)\n",
    "\n",
    "The below presents a toy example illustrating how an iteration of the algorithm would operate in practice. Five examples are provided in this toy training set with three features, two continuous variables representing the total amount of rainfall on the day of the flight and average visibility on the day of the flight (both normalized to values between 0 and 1) and a dummy variable representing time of flight (=1 for PM, =0 for AM). As in our dataset, a label = 1 means a delayed flight and a label = 0 means an ontime flight.\n",
    "\n",
    "| \\\\(i\\\\) | \\\\(x\\_{1i}\\\\) |    \\\\(x\\_{2i}\\\\)  |    \\\\(x\\_{3i}\\\\)  |   \\\\(y\\_i\\\\) | \n",
    "|:---:|:--------:|:---------------:|\n",
    "|  -  | PM |  Rain  | Visibility | label |  \n",
    "| 1     |1     |     0.0     |     1     |     0     |       \n",
    "| 2     |0     |     0.0     |     0.9     |     0     |       \n",
    "| 3     |1     |     0.1     |     0.5     |     1     |      \n",
    "| 4     |0     |     0.1     |     1     |     0     |     \n",
    "| 5     |1     |     1.0     |     0.2     |     1     |       \n",
    "\n",
    "The parameter vector \\\\(\\theta\\\\) for our initial line \\\\(y=x \\\\) is  \\\\(\\theta\\\\) = \\\\(\\begin{bmatrix} 1 \\ \\quad 1 \\ \\quad 1 \\ \\quad 0 \\ \\end{bmatrix} \\\\) where the last value is the constant. \n",
    "         \n",
    "The (augmented) data points \\\\(x_j\\\\) are:\n",
    "\\\\( \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 1 \\\\\\ 1 \\\\\\ \\end{bmatrix} \\ \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ .9 \\\\\\ 1 \\\\\\ \\end{bmatrix} \\ \\begin{bmatrix} 1 \\\\\\ .1 \\\\\\ .5 \\\\\\ 1 \\\\\\ \\end{bmatrix} \\ \\begin{bmatrix} 0 \\\\\\ .1 \\\\\\ 1 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\ \\begin{bmatrix} 1 \\\\\\ 1 \\\\\\ .2 \\\\\\ 1 \\\\\\ \\end{bmatrix} \\\\)\n",
    "\n",
    "The following table provides the values required to calculate the loss as well as the gradient update required in the first iteration of gradient descent. \n",
    "\n",
    "| \\\\(i\\\\) |  \\\\(x\\_j'\\\\)  | \\\\(y\\_j\\\\) |   \\\\(\\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_j\\\\) | \\\\(h_ \\theta (x) = g(\\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_j\\\\)) | <img src=\"https://github.com/tonydisera/261-final-project-images/raw/master//LR%20individual%20loss.jpg?raw=true\" width=60%/> | <img src=\"https://github.com/tonydisera/261-final-project-images/raw/master/LR%20individual%20gradient.jpg?raw=true\" width=60%/> |\n",
    "|:----:|:-----:|:----------------:|:------------------------:|:------------------------:|\n",
    "|  -  |  input   | true \\\\(y\\\\)   |   linear weights    |  predicted probability       |loss  component for \\\\(x\\_j\\\\)       |gradient  component for \\\\(x\\_j\\\\)       \n",
    "| 1     | \\\\( \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 1 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\\\)   |  0   |     2.0             |    0.1192    |    0.1269    |\\\\( \\begin{bmatrix} 0.12 \\\\\\ 0 \\\\\\ 0.12 \\\\\\ 0.12 \\\\\\ \\end{bmatrix}\\\\)   |      \n",
    "| 2     | \\\\( \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ .9 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\\\)   |  0   |     0.9             |    0.2891  |    0.3412    |\\\\( \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ 0.26 \\\\\\ 0.29 \\\\\\ \\end{bmatrix}\\\\)   |   \n",
    "| 3     | \\\\( \\begin{bmatrix} 1 \\\\\\ .1 \\\\\\ .5 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\\\)   |  1   |     1.6             |   0.1680  |    1.7839    |\\\\( \\begin{bmatrix} -0.83 \\\\\\ -0.08 \\\\\\ -0.42 \\\\\\ -0.83 \\\\\\ \\end{bmatrix}\\\\)   |   \n",
    "| 4     | \\\\( \\begin{bmatrix} 0 \\\\\\ .1 \\\\\\ 1 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\\\)   |  0   |     1.1             |    0.2497  |    0.2873    |\\\\( \\begin{bmatrix} 0 \\\\\\ 0.02 \\\\\\ 0.25 \\\\\\ 0.25 \\\\\\ \\end{bmatrix}\\\\)   |   \n",
    "| 5     | \\\\( \\begin{bmatrix} 1 \\\\\\ 1 \\\\\\ .2 \\\\\\ 1 \\\\\\ \\end{bmatrix}\\\\)   |  1   |     2.2             |    0.0998  |    2.3051    |\\\\( \\begin{bmatrix} -0.90 \\\\\\ -0.90 \\\\\\ -0.18 \\\\\\ -0.90 \\\\\\ \\end{bmatrix}\\\\)   | \n",
    "\n",
    "\n",
    "The training loss \\\\(f(\\boldsymbol{\\theta})\\\\) for this data and these weights is: 0.1269 + 0.3412 + 1.7839 + 0.2873 +  2.3051 = 4.8444 / 5 = 0.96888\n",
    "\n",
    "Our gradient update is \\\\( \\begin{bmatrix} -1.61 \\\\\\ -0.96 \\\\\\ 0.03 \\\\\\ -1.07 \\\\\\ \\end{bmatrix} \\\\) and assuming a learning rate of \\\\(\\eta = \\\\)  0.1, we would adjust our initial weight matrix by \\\\( \\begin{bmatrix} -0.16 \\\\\\ -0.10 \\\\\\ 0.003 \\\\\\ -0.11 \\\\\\ \\end{bmatrix} \\\\)\n",
    "\n",
    "Our new weight matrix would therefore be \\\\( \\begin{bmatrix} 1.16 \\\\\\ 1.10 \\\\\\ 1.00 \\\\\\ 0.11 \\\\\\ \\end{bmatrix} \\\\)\n",
    "\n",
    "This process would continue until the the difference between the new weight matrix and the prior weight matrix becomes sufficiently small (i.e. where the derivative reaches nearly zero) that we can assume the minimum of the loss is reached and the optimal weight matrix has been approximated. The application of logistic regression in Spark MLlib differs from standard gradient descent which was applied above, since it would be extremely computational expensive to perform these calculations on our entire training set. Therefore, in practice stochastic gradient descent is applied. In this case only some of the examples, rather than the entire training set, are used to calculate the gradient that informs each step change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Algorithm Implementation\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/ensemble.png?raw=true\" width=\"30%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression - Hyper Parameter Tuning using Gridsearch and Crossfold Validation\n",
    "\n",
    "Once both the weighted and unweighted models were established, we then explored different versions of hyperparameter tuning using gridsearch and crossfold validation. We set the validation metric to be the area under the precision-recall curve (AUC PR). This metric has the dual advantages of both being easily accessible by the API (and thus suitable to implement with crossfold validation) and similar in terms of scale and the directions of movement to the F1 metric that is our primary outcome metric. The crossfold validation exercise is computationally expensive and takes a long time, so we created a small sample (5%) of the training set and performed the crossfold validation on this smaller dataset.  \n",
    "The code blocks are included below but commented out since this is a very time consuming operation. From previous work, the optimal parameters as identified by gridsearch with crossfold validation are:  \n",
    "* **elasticNetParam (\\\\(\\alpha)\\\\)** : 0.25\n",
    "* **regParam (\\\\(\\lambda)\\\\)** : 0.001\n",
    "* **maxIter** : 20\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/reg.png?raw=true\" width=\"30%\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CODE COMMENTED TO AVOID RE-RUNNING\n",
    "# #For this iteration, we had already determined that weighted models performed better so we did not search on that hyperparameter to save computation time.\n",
    "# # Set the baseline model off of which the parameter grid can be builtbest_LRparams = cvModel.bestModel.extractParamMap()\n",
    "# lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",  weightCol=\"weight\", standardization=False)\n",
    "\n",
    "# # Create ParamGrid for Cross Validation\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(lr.regParam, [0, .001, 0.01])\n",
    "#              .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 1.0])\n",
    "#              .addGrid(lr.maxIter, [20, 50])\n",
    "#              .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CODE COMMENTED TO AVOID RE-RUNNING\n",
    "\n",
    "# # Set the evaluator function for the CrossValidator\n",
    "# evaluator = BinaryClassificationEvaluator()\n",
    "# evaluator = evaluator.setMetricName('areaUnderPR')\n",
    "\n",
    "# # Create 5-fold CrossValidator\n",
    "# cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# print(\"LR Model - evaluation against dev set\")\n",
    "# print(\"========================\")\n",
    "# start = time.time()\n",
    "\n",
    "# # Run cross validations on the small training set\n",
    "# cvModel = cv.fit(data_train_small)\n",
    "# print(f\"\\nTrained gridsearch crossfold validation model in {(time.time() - start)/60} minutes\")\n",
    "# predictions = cvModel.transform(data_dev).cache()\n",
    "# predictions.createOrReplaceTempView(\"predictions\")\n",
    "# evaluatePerformance(predictions, \"Gridsearch_Crossfold_Validation_LR\")\n",
    "# print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CODE COMMENTED TO AVOID RE-RUNNING\n",
    "\n",
    "# # Extract and view the set of parameters returned for the best model in the crossfold validation set.\n",
    "# best_LRparams = cvModel.bestModel.extractParamMap()\n",
    "# best_LRparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression with Optimal Parameters (Identified by Gridsearch and Crossfold Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the optimal hyper paramters from the previous section, we can now use these parameter (regParam, elasticNetParam, maxIter) in subsequent models, starting with a single logistic regression model trained on the full dataset (with class weights). Results will show a slight improvement over the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Logistic Regression Model [weighted] with optimal parameters </span>\n",
       "\n",
       "Trained model in 0.6888488014539083 minutes\n",
       "\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t424,776\n",
       "tn (correct   pred as ontime)\t1,705,778\n",
       "fp (incorrect pred as delay) \t938,075\n",
       "fn (incorrect pred as ontime)\t242,810, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6434 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3117 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6363 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4184\n",
       "Area under PR = 0.5589443795589635\n",
       "\n",
       "Evaluated metrics in 0.35299224058787027 minutes\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create LogisticRegression model with class weighted on full training dataset (no temporal data)\n",
    "# Use optimal hyper parameters as identified by gridsearch with crossfold\n",
    "\n",
    "lrBestParams = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\", maxIter=20, standardization=False, regParam=.001, elasticNetParam=0.25 )\n",
    "\n",
    "# Train model with Training Data\n",
    "\n",
    "start = time.time()\n",
    "lrBestParamsModel = lrBestParams.fit(data_train)\n",
    "print(\"\\033[1m Logistic Regression Model [weighted] with optimal parameters \\033[0m\")\n",
    "\n",
    "print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"\\nEvaluation against dev set\")\n",
    "print(\"========================\")\n",
    "start = time.time()\n",
    "predictions = lrBestParamsModel.transform(data_dev).cache()\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "evaluatePerformance(predictions, \"LR_Weighted_OptimalParms\")\n",
    "print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Final Model Selected : Ensemble Model using Logistic Regression and Temporal Data\n",
    "\n",
    "<img src=\"https://github.com/tonydisera/261-final-project-images/blob/master/multimodel.png?raw=true\" width=\"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have gained insight into the fact that logistic regression with class weights and optimal parameters found using gridsearch and cross validation have outperformed other models, including unweighted logistic regression and, to our great surprise, tree based models. Thus having selected logistic regression as our algorithm of choice, we now shift our focus towards getting additional model performance gains.  \n",
    "Given that there is evidence of delays varying based on seasonality and arrival time of day as described in the EDA section 2.2 above, we tried ensemble models that combine the decisions from multiple time-stratified models to improve the overall performance.  We decided to train individual models based on one or more aspects of temporal data and feed dev/test data to the appropriate model for delay or no delay prediction. The temporal properties of interest are: \n",
    "  \n",
    "**TIME-OF-DAY** = ['LATE_NIGHT', 'MORNING', 'EVENING']  \n",
    "**SEASONS** = ['WINTER', 'SPRING', 'SUMMER', 'FALL']  \n",
    "\n",
    "In order to train the multiple models, we first need to add the appropriate flags to the data (train, dev and test) and divide the training set into appropriate subsets. These subsets will then be used to train the indivdual models that make up the ensemble. Similarly, the dev (and test) observations need to be sent to the corresponding model for prediction. And finally, results from each model will be collected to give overall model metrics.  \n",
    "  \n",
    "We tried several variants of ensemble models e.g.\n",
    "- Season based : 4 models --> 'WINTER', 'SPRING', 'SUMMER', 'FALL'  \n",
    "- Time based: 3 models --> 'LATE_NIGHT', 'MORNING', 'EVENING'  \n",
    "- Season + Time based: 12 models --> 'WINTER-LATE_NIGHT', 'WINTER-MORNING', 'WINTER-EVENING', 'SPRING-LATE_NIGHT', 'SPRING-MORNING', 'SPRING-EVENING', 'SUMMER-LATE_NIGHT', 'SUMMER-MORNING', 'SUMMER-EVENING', 'FALL-LATE_NIGHT', 'FALL-MORNING', 'FALL-EVENING'  \n",
    "\n",
    "In the interest of brevity, we have only provided code (output) for the final ensemble model [ Season + Time based: 12 models] as shown in the figure below.  \n",
    "This model provided better results than ensemble models of just Seasons or just Time-of-day and of course, all the previous models shown above.  \n",
    "  \n",
    "From a scalability perspective, we believe that the parallel nature of Spark's machine learing API (ML Lib) works very well for such large datasets. The models train fairly quickly, with the ensemble only taking a few minutes. This is despite the fact that while the underlying MLLib data processing engine is distributed in nature, in our current implemenation, we train and evaluate the individual models themselves in series. Given more time, we would have liked to use the Spark framework to further parallelize the training and evaluation of the individual models as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Add Season and Arrival Time of Day column to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add relevant columns to train, dev and test dataset\n",
    "# Train Data\n",
    "data_train = data_train.withColumn('SEASON', f.when((f.col(\"MONTH\") >= 3) & (f.col(\"MONTH\") <= 5), \"SPRING\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 6) & (f.col(\"MONTH\") <= 8), \"SUMMER\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 9) & (f.col(\"MONTH\") <= 11), \"FALL\")\\\n",
    "                                          .otherwise(\"WINTER\"))\n",
    "\n",
    "data_train = data_train.withColumn('ARR_TIME_OF_DAY', f.when((f.col(\"ARR_TIME_BLK\") == '0001-0559') | (f.col(\"ARR_TIME_BLK\") == '2200-2259')| (f.col(\"ARR_TIME_BLK\") == '2300-2359'), \"LATE_NIGHT\")\\\n",
    "                                          .when((f.col(\"ARR_TIME_BLK\") == '0600-0659') | (f.col(\"ARR_TIME_BLK\") == '0700-0759')| (f.col(\"ARR_TIME_BLK\") == '0800-0859'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '0900-0959') | (f.col(\"ARR_TIME_BLK\") == '1000-1059')| (f.col(\"ARR_TIME_BLK\") == '1100-1159'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '1200-1259') | (f.col(\"ARR_TIME_BLK\") == '1300-1359'), \"MORNING\")\\\n",
    "                                          .otherwise(\"EVENING\"))\n",
    "# Dev Data\n",
    "data_dev = data_dev.withColumn('SEASON', f.when((f.col(\"MONTH\") >= 3) & (f.col(\"MONTH\") <= 5), \"SPRING\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 6) & (f.col(\"MONTH\") <= 8), \"SUMMER\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 9) & (f.col(\"MONTH\") <= 11), \"FALL\")\\\n",
    "                                          .otherwise(\"WINTER\"))\n",
    "\n",
    "data_dev = data_dev.withColumn('ARR_TIME_OF_DAY', f.when((f.col(\"ARR_TIME_BLK\") == '0001-0559') | (f.col(\"ARR_TIME_BLK\") == '2200-2259')| (f.col(\"ARR_TIME_BLK\") == '2300-2359'), \"LATE_NIGHT\")\\\n",
    "                                          .when((f.col(\"ARR_TIME_BLK\") == '0600-0659') | (f.col(\"ARR_TIME_BLK\") == '0700-0759')| (f.col(\"ARR_TIME_BLK\") == '0800-0859'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '0900-0959') | (f.col(\"ARR_TIME_BLK\") == '1000-1059')| (f.col(\"ARR_TIME_BLK\") == '1100-1159'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '1200-1259') | (f.col(\"ARR_TIME_BLK\") == '1300-1359'), \"MORNING\")\\\n",
    "                                          .otherwise(\"EVENING\"))\n",
    "# Test Data\n",
    "data_test = data_test.withColumn('SEASON', f.when((f.col(\"MONTH\") >= 3) & (f.col(\"MONTH\") <= 5), \"SPRING\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 6) & (f.col(\"MONTH\") <= 8), \"SUMMER\")\\\n",
    "                                          .when((f.col(\"MONTH\") >= 9) & (f.col(\"MONTH\") <= 11), \"FALL\")\\\n",
    "                                          .otherwise(\"WINTER\"))\n",
    "\n",
    "data_test = data_test.withColumn('ARR_TIME_OF_DAY', f.when((f.col(\"ARR_TIME_BLK\") == '0001-0559') | (f.col(\"ARR_TIME_BLK\") == '2200-2259')| (f.col(\"ARR_TIME_BLK\") == '2300-2359'), \"LATE_NIGHT\")\\\n",
    "                                          .when((f.col(\"ARR_TIME_BLK\") == '0600-0659') | (f.col(\"ARR_TIME_BLK\") == '0700-0759')| (f.col(\"ARR_TIME_BLK\") == '0800-0859'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '0900-0959') | (f.col(\"ARR_TIME_BLK\") == '1000-1059')| (f.col(\"ARR_TIME_BLK\") == '1100-1159'), \"MORNING\")\\\n",
    "\t\t\t\t\t\t\t\t\t\t  .when((f.col(\"ARR_TIME_BLK\") == '1200-1259') | (f.col(\"ARR_TIME_BLK\") == '1300-1359'), \"MORNING\")\\\n",
    "                                          .otherwise(\"EVENING\"))\n",
    "\n",
    "data_train.createOrReplaceTempView(\"data_train\")\n",
    "data_dev.createOrReplaceTempView(\"data_dev\")\n",
    "data_test.createOrReplaceTempView(\"data_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Split Train, Dev and Test Data by Season-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> ROW COUNTS PER DATA SUBSET </span>\n",
       "<span class=\"ansi-bold\"> WINTER-LATE_NIGHT </span>\n",
       "data_trainWINTER_LATE_NIGHT:  702,744\n",
       "data_devWINTER_LATE_NIGHT:  67,095\n",
       "data_testWINTER_LATE_NIGHT:  67,525\n",
       "<span class=\"ansi-bold\"> WINTER-MORNING </span>\n",
       "data_trainWINTER_MORNING:  2,418,705\n",
       "data_devWINTER_MORNING:  221,417\n",
       "data_testWINTER_MORNING:  220,890\n",
       "<span class=\"ansi-bold\"> WINTER-EVENING </span>\n",
       "data_trainWINTER_EVENING:  3,043,679\n",
       "data_devWINTER_EVENING:  272,118\n",
       "data_testWINTER_EVENING:  271,624\n",
       "<span class=\"ansi-bold\"> SPRING-LATE_NIGHT </span>\n",
       "data_trainSPRING_LATE_NIGHT:  721,398\n",
       "data_devSPRING_LATE_NIGHT:  113,478\n",
       "data_testSPRING_LATE_NIGHT:  113,617\n",
       "<span class=\"ansi-bold\"> SPRING-MORNING </span>\n",
       "data_trainSPRING_MORNING:  2,388,670\n",
       "data_devSPRING_MORNING:  365,139\n",
       "data_testSPRING_MORNING:  365,820\n",
       "<span class=\"ansi-bold\"> SPRING-EVENING </span>\n",
       "data_trainSPRING_EVENING:  2,935,502\n",
       "data_devSPRING_EVENING:  439,522\n",
       "data_testSPRING_EVENING:  439,073\n",
       "<span class=\"ansi-bold\"> SUMMER-LATE_NIGHT </span>\n",
       "data_trainSUMMER_LATE_NIGHT:  773,203\n",
       "data_devSUMMER_LATE_NIGHT:  117,662\n",
       "data_testSUMMER_LATE_NIGHT:  118,197\n",
       "<span class=\"ansi-bold\"> SUMMER-MORNING </span>\n",
       "data_trainSUMMER_MORNING:  2,506,339\n",
       "data_devSUMMER_MORNING:  380,120\n",
       "data_testSUMMER_MORNING:  380,350\n",
       "<span class=\"ansi-bold\"> SUMMER-EVENING </span>\n",
       "data_trainSUMMER_EVENING:  3,023,505\n",
       "data_devSUMMER_EVENING:  448,353\n",
       "data_testSUMMER_EVENING:  448,186\n",
       "<span class=\"ansi-bold\"> FALL-LATE_NIGHT </span>\n",
       "data_trainFALL_LATE_NIGHT:  658,570\n",
       "data_devFALL_LATE_NIGHT:  104,384\n",
       "data_testFALL_LATE_NIGHT:  104,508\n",
       "<span class=\"ansi-bold\"> FALL-MORNING </span>\n",
       "data_trainFALL_MORNING:  2,333,219\n",
       "data_devFALL_MORNING:  356,481\n",
       "data_testFALL_MORNING:  356,261\n",
       "<span class=\"ansi-bold\"> FALL-EVENING </span>\n",
       "data_trainFALL_EVENING:  2,869,527\n",
       "data_devFALL_EVENING:  425,670\n",
       "data_testFALL_EVENING:  426,497\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize empty dictionary to store dataset names & reference dataframes [train, dev, test]\n",
    "trainSetDictionaryCombined = {}\n",
    "devSetDictionaryCombined = {}\n",
    "testSetDictionaryCombined = {}\n",
    "\n",
    "# Training data is split into multiple datasets based on Season and Arrival Time of Day \n",
    "# Since there 12 dataset names based on combinations, the dataset names are stored in a dictionary referecing the appropriate dataframe\n",
    "\n",
    "TIME = [\"LATE_NIGHT\", \"MORNING\", \"EVENING\"]\n",
    "SEASONS = ['WINTER', 'SPRING', 'SUMMER', 'FALL']\n",
    "\n",
    "print(\"\\033[1m ROW COUNTS PER DATA SUBSET \\033[0m\")\n",
    "for s in SEASONS:\n",
    "  for t in TIME:\n",
    "    trainQuery = \"select * from data_train where SEASON = '{}' and ARR_TIME_OF_DAY = '{}'\".format(s,t)\n",
    "    trainSetName = 'data_train' + s + \"_\" + t\n",
    "    trainSetNameStr = str(trainSetName)\n",
    "    trainSetName = spark.sql(trainQuery)\n",
    "    trainSetName.createOrReplaceTempView(trainSetNameStr)\n",
    "    print(\"\\033[1m {}-{} \\033[0m\".format(s,t))\n",
    "    print('{}:  {:,}'.format(trainSetNameStr, trainSetName.count()))\n",
    "    trainSetDictionaryCombined[trainSetNameStr] = trainSetName\n",
    "    # Get delay / no delay weights specifically for subset of Season-Time training data. If commented out then weights from full training set will be applied to data subsets\n",
    "#     weight_nodelay,weight_delay = getWeight(trainSetDictionaryCombined[trainSetNameStr])\n",
    "#     trainSetDictionaryCombined[trainSetNameStr] = setWeight(trainSetDictionaryCombined[trainSetNameStr])\n",
    "    devQuery = \"select * from data_dev where SEASON = '{}' and ARR_TIME_OF_DAY = '{}'\".format(s,t)\n",
    "    devSetName = 'data_dev' + s + \"_\" + t\n",
    "    devSetNameStr = str(devSetName)\n",
    "    devSetName = spark.sql(devQuery)\n",
    "    devSetName.createOrReplaceTempView(devSetNameStr)\n",
    "    print('{}:  {:,}'.format(devSetNameStr, devSetName.count()))\n",
    "    devSetDictionaryCombined[devSetNameStr] = devSetName\n",
    "#     devSetDictionaryCombined[devSetNameStr] = setWeight(devSetDictionaryCombined[devSetNameStr])\n",
    "\n",
    "    testQuery = \"select * from data_test where SEASON = '{}' and ARR_TIME_OF_DAY = '{}'\".format(s,t)\n",
    "    testSetName = 'data_test' + s + \"_\" + t\n",
    "    testSetNameStr = str(testSetName)\n",
    "    testSetName = spark.sql(testQuery)\n",
    "    testSetName.createOrReplaceTempView(testSetNameStr)\n",
    "    print('{}:  {:,}'.format(testSetNameStr, testSetName.count()))\n",
    "    testSetDictionaryCombined[testSetNameStr] = testSetName\n",
    "#     testSetDictionaryCombined[testSetNameStr] = setWeight(testSetDictionaryCombined[testSetNameStr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Final Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3.1 Evaluation on  Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Ensemble Model based on Season-Time </span>\n",
       "<span class=\"ansi-bold\"> --- Logistic Regression Model [weighted] with optimal parameters </span>\n",
       "WINTER_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.2473706324895223 minutes\n",
       "========================\n",
       "WINTER_MORNING\n",
       "\n",
       "Trained model in 0.15380523204803467 minutes\n",
       "========================\n",
       "WINTER_EVENING\n",
       "\n",
       "Trained model in 0.1844387412071228 minutes\n",
       "========================\n",
       "SPRING_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.12212580442428589 minutes\n",
       "========================\n",
       "SPRING_MORNING\n",
       "\n",
       "Trained model in 0.15077107747395832 minutes\n",
       "========================\n",
       "SPRING_EVENING\n",
       "\n",
       "Trained model in 0.16874881585439047 minutes\n",
       "========================\n",
       "SUMMER_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.1527131716410319 minutes\n",
       "========================\n",
       "SUMMER_MORNING\n",
       "\n",
       "Trained model in 0.15750839312871298 minutes\n",
       "========================\n",
       "SUMMER_EVENING\n",
       "\n",
       "Trained model in 0.16229557196299235 minutes\n",
       "========================\n",
       "FALL_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.12270195086797078 minutes\n",
       "========================\n",
       "FALL_MORNING\n",
       "\n",
       "Trained model in 0.1553865909576416 minutes\n",
       "========================\n",
       "FALL_EVENING\n",
       "\n",
       "Trained model in 0.17851425011952718 minutes\n",
       "========================\n",
       "Evaluation against dev set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t445,255\n",
       "tn (correct   pred as ontime)\t1,657,983\n",
       "fp (incorrect pred as delay) \t985,870\n",
       "fn (incorrect pred as ontime)\t222,331, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6351 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3111 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.667 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4243\n",
       "Area under PR = 0.5860931414219752\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create weighted LogisticRegression models by Season-Time with class weights \n",
    "# Use optimal hyper parameters as identified by gridsearch with crossfold\n",
    "\n",
    "newDF_Flag = 0\n",
    "lrWeighted = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\", maxIter=20, standardization=False, regParam=0.001, elasticNetParam=0.25 )\n",
    "print(\"\\033[1m Ensemble Model based on Season-Time \\033[0m\")\n",
    "print(\"\\033[1m --- Logistic Regression Model [weighted] with optimal parameters \\033[0m\")\n",
    "for s in SEASONS:\n",
    "  for t in TIME:\n",
    "    start = time.time()\n",
    "  # Train model with Training Data\n",
    "    lrModel = lrWeighted.fit(trainSetDictionaryCombined['data_train' + s + \"_\" + t])\n",
    "    print(s + \"_\" + t)\n",
    "    print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "    print(\"========================\")\n",
    "    start = time.time()\n",
    "    predictions = lrModel.transform(devSetDictionaryCombined['data_dev' + s + \"_\" + t]).cache()\n",
    "    # Must create predictionsAll DF to append results from each individual model in the ensemble. Empty DF must be created with correct schema matching predictions DF from model\n",
    "    if newDF_Flag == 0:\n",
    "      schema = predictions.schema\n",
    "      predictionsAll = spark.createDataFrame([], schema)\n",
    "      newDF_Flag = 1\n",
    "    predictionsAll = predictionsAll.union(predictions)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    modelName = 'LR_Weighted_CV_Parms' + s.capitalize() + t.capitalize()\n",
    "#     evaluatePerformance(predictions, modelName+'_0', 'LR_Weighted_CV_Parms_SeasonTime_regParm_0')\n",
    "#     print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"Evaluation against dev set\")\n",
    "print(\"========================\")\n",
    "evaluatePerformance(predictionsAll, \"LR_Weighted_OptimalParms_MultiModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pd.DataFrame(predictionDF.loc['LR_Weighted_OptimalParms_MultiModel']).transpose()\n",
    "# predictionDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3.2 Evaluation on Unseen Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-bold\"> Ensemble Model based on Season-Time </span>\n",
       "<span class=\"ansi-bold\"> --- Logistic Regression Model [weighted] with optimal parameters </span>\n",
       "WINTER_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.25738542874654136 minutes\n",
       "========================\n",
       "WINTER_MORNING\n",
       "\n",
       "Trained model in 0.15558274586995444 minutes\n",
       "========================\n",
       "WINTER_EVENING\n",
       "\n",
       "Trained model in 0.17821288506189983 minutes\n",
       "========================\n",
       "SPRING_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.11240336894989014 minutes\n",
       "========================\n",
       "SPRING_MORNING\n",
       "\n",
       "Trained model in 0.14254608551661174 minutes\n",
       "========================\n",
       "SPRING_EVENING\n",
       "\n",
       "Trained model in 0.1795898954073588 minutes\n",
       "========================\n",
       "SUMMER_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.2530485153198242 minutes\n",
       "========================\n",
       "SUMMER_MORNING\n",
       "\n",
       "Trained model in 0.15745524565378824 minutes\n",
       "========================\n",
       "SUMMER_EVENING\n",
       "\n",
       "Trained model in 0.17379790147145588 minutes\n",
       "========================\n",
       "FALL_LATE_NIGHT\n",
       "\n",
       "Trained model in 0.11527680158615113 minutes\n",
       "========================\n",
       "FALL_MORNING\n",
       "\n",
       "Trained model in 0.13977174758911132 minutes\n",
       "========================\n",
       "FALL_EVENING\n",
       "\n",
       "Trained model in 0.16040164629618328 minutes\n",
       "========================\n",
       "Evaluation against test set\n",
       "========================\n",
       "Summary Stats\n",
       "tp (correct   pred as delay) \t444,123\n",
       "tn (correct   pred as ontime)\t1,659,791\n",
       "fp (incorrect pred as delay) \t986,377\n",
       "fn (incorrect pred as ontime)\t222,257, &#34;\n",
       "&#34;\n",
       "accuracy \t0.6351 \tout of all observations, how many were predicted correctly?\n",
       "precision\t0.3105 \tout of all delays predicted, how many are correct?\n",
       "recall   \t0.6665 \tout of all actual delays, how many are correctly predicted?\n",
       "f1       \t0.4236\n",
       "Area under PR = 0.5855791068574537\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation on Test Data\n",
    "# Create weighted LogisticRegression models by Season-Time with class weights \n",
    "# Use optimal hyper parameters as identified by gridsearch with crossfold\n",
    "\n",
    "newDF_Flag = 0\n",
    "\n",
    "lrWeighted = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\", maxIter=20, standardization=False, regParam=0.001, elasticNetParam=0.25 )\n",
    "print(\"\\033[1m Ensemble Model based on Season-Time \\033[0m\")\n",
    "print(\"\\033[1m --- Logistic Regression Model [weighted] with optimal parameters \\033[0m\")\n",
    "for s in SEASONS:\n",
    "  for t in TIME:\n",
    "    start = time.time()\n",
    "  # Train model with Training Data\n",
    "    lrModel = lrWeighted.fit(trainSetDictionaryCombined['data_train' + s + \"_\" + t])\n",
    "    print(s + \"_\" + t)\n",
    "    print(f\"\\nTrained model in {(time.time() - start)/60} minutes\")\n",
    "    print(\"========================\")\n",
    "    start = time.time()\n",
    "    predictions = lrModel.transform(testSetDictionaryCombined['data_test' + s + \"_\" + t]).cache()\n",
    "    # Must create predictionsAll DF to append results from each individual model in the ensemble. Empty DF must be created with correct schema matching predictions DF from model\n",
    "    if newDF_Flag == 0:\n",
    "      schema = predictions.schema\n",
    "      predictionsFinal = spark.createDataFrame([], schema)\n",
    "      newDF_Flag = 1\n",
    "    predictionsFinal = predictionsFinal.union(predictions)\n",
    "    predictions.createOrReplaceTempView(\"predictions\")\n",
    "    modelName = 'Test Data_' + s.capitalize() + t.capitalize()\n",
    "#     evaluatePerformance(predictions, modelName+'_0', 'LR_Weighted_CV_Parms_SeasonTime_regParm_0')\n",
    "#     print(f\"\\nEvaluated metrics in {(time.time() - start)/60} minutes\")\n",
    "\n",
    "print(\"Evaluation against test set\")\n",
    "print(\"========================\")\n",
    "evaluatePerformance(predictionsFinal, \"TestData_Eval_LR_Weighted_OptimalParms_MultiModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our models are able to correctly predict (true positive) a negative arrival experience (including delayed arrivals >15 minutes, diverted or cancelled flights) with a recall of .67 and an f1-score of .42. This result holds for all flights across the United States, from the busiest airports to the smallest. Our model struggles the most with over-predicting delays, which is apparent in the relatively poor precision scores. We continue to believe that customers are most interested in correctly predicting when a flight might be delayed, and would react to our error in precision as \"getting lucky\", much the way one might feel when rain is predicted but never materializes. While there are clearly examples in the literature that perform better, closer inspection frequently reveals that those problem sets have been more finely specified by only focusing on select cities or select airlines. Moreover, performance comparison between models and confusion matrix for the final ensemble model are shown in the sections below.\n",
    "\n",
    "In terms of machine learning items, some of our team's most significant learnings include:\n",
    "- The tuning decision we made that had the largest impact across all models was creating a balanced training set.\n",
    "- The process of writing the bagging algorithm from scratch really helped us understand how it worked.\n",
    "- We seemed to get \"stuck\" in a relatively narrow performance range with all of our tree-based models. Spark's MLLib API is relatively limited, and by the time we got our bagging algorithm working we had limited time to troubleshoot this. Given more time, we could have pulled a sample of the data out of Spark and experimented with the more versatile tools in SKLearn and then applied that knowledge to the larger dataset. Additionally, we could have explored our feature engineering approach to see if this potential \"underfitting\" could be resolved with additional features (see point below on weather).\n",
    "- The weather data was extremely complex and quite messy. We had to make a number of decisions along the way to get the model working. Our intent had been to get a base level of weather data incorporated, then circle back and add more. We only had time to add in snow data during that second pass.\n",
    "- We would have liked to further refine our outcome model. While signaling to a customer than an adverse effect is likely is a good thing, there remains a large gap in outcomes between a 16 minute delay and a four hour delay (or diverted flight!). Given more time, we would have liked to try to refine that outcome prediction with an additional model.\n",
    "\n",
    "In terms of scalability, some of our most significant learnings include:\n",
    " - The use of the MLLib API abstracts away much of the thought (and frustration!) we experienced during the course working with RDDs. Indeed, in the rare occasions during this project where we considered utilising RDDs, out of memory errors quickly refocused us on finding solutions within the Dataframe API.\n",
    " - There remained a couple of instances, namely in creating and training our bagged models and in training the logistic regression ensembles, where we did not take advantage of the distributed environment. Given more time, we might have realized performance gains by parallelizing those computations, rather than iterating through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load savaed model metrics from CSV\n",
    "\n",
    "metrics_df = pd.read_csv(\"/dbfs/user/ammara.essa@ischool.berkeley.edu/W261_Final_Project/predictions_df.csv\")\n",
    "metrics_df.set_index('Unnamed: 0', inplace=True)\n",
    "metrics_df.rename(index={'Unnamed: 0' : 'Model Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrix for Test Data  \n",
    "Below is the confusion matrix for predictions on the unseen test data. As we can see, there are still quite a few false negatives in the model, indicating that fligts that are delayed are still being classified as not delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_99c43eb8_80f2_11ea_9380_00163e1d8677row0_col0 {\n",
       "            background-color:  #e5e5ff;\n",
       "            color:  #000000;\n",
       "        }    #T_99c43eb8_80f2_11ea_9380_00163e1d8677row0_col1 {\n",
       "            background-color:  #e5e5ff;\n",
       "            color:  #000000;\n",
       "        }    #T_99c43eb8_80f2_11ea_9380_00163e1d8677row1_col0 {\n",
       "            background-color:  #0000ff;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_99c43eb8_80f2_11ea_9380_00163e1d8677row1_col1 {\n",
       "            background-color:  #0000ff;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Delay_(Predicted)</th>        <th class=\"col_heading level0 col1\" >No_Delay_(Predicted)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677level0_row0\" class=\"row_heading level0 row0\" >Delay_(Actual)</th>\n",
       "                        <td id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677row0_col0\" class=\"data row0 col0\" >444123</td>\n",
       "                        <td id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677row0_col1\" class=\"data row0 col1\" >222257</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677level0_row1\" class=\"row_heading level0 row1\" >No_Delay_(Actual)</th>\n",
       "                        <td id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677row1_col0\" class=\"data row1 col0\" >986377</td>\n",
       "                        <td id=\"T_99c43eb8_80f2_11ea_9380_00163e1d8677row1_col1\" class=\"data row1 col1\" >1659791</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\033[1m Confusion matrix for model : TestData_Eval_LR_Weighted_OptimalParms_MultiModel \\033[0m')\n",
    "showConfusionMatrix(metrics_df.loc['TestData_Eval_LR_Weighted_OptimalParms_MultiModel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comparison of Models  \n",
    "We've also provided a comparison of all the aforementioned models (in both tabular and graphical form), concluding with our final model ***LR_Weighted_OptimalParms_MultiModel*** for which metrics against dev and test data are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col0 {\n",
       "            background-color:  #8ecf67;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col1 {\n",
       "            background-color:  #be1827;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col2 {\n",
       "            background-color:  #e54e35;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col0 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col1 {\n",
       "            background-color:  #b10b26;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col2 {\n",
       "            background-color:  #d02927;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col0 {\n",
       "            background-color:  #4eb15d;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col1 {\n",
       "            background-color:  #ab0626;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col2 {\n",
       "            background-color:  #bb1526;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col0 {\n",
       "            background-color:  #ad0826;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col1 {\n",
       "            background-color:  #199750;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col2 {\n",
       "            background-color:  #118848;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col0 {\n",
       "            background-color:  #db382b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col1 {\n",
       "            background-color:  #3faa59;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col2 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col0 {\n",
       "            background-color:  #b10b26;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col1 {\n",
       "            background-color:  #118848;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col2 {\n",
       "            background-color:  #0c7f43;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col0 {\n",
       "            background-color:  #b30d26;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col1 {\n",
       "            background-color:  #219c52;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col2 {\n",
       "            background-color:  #108647;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col0 {\n",
       "            background-color:  #bd1726;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col1 {\n",
       "            background-color:  #108647;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col2 {\n",
       "            background-color:  #06733d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col0 {\n",
       "            background-color:  #b30d26;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col1 {\n",
       "            background-color:  #128a49;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col2 {\n",
       "            background-color:  #0c7f43;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col0 {\n",
       "            background-color:  #c7e77f;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col1 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col2 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col0 {\n",
       "            background-color:  #ad0826;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col1 {\n",
       "            background-color:  #118848;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col2 {\n",
       "            background-color:  #0e8245;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col1 {\n",
       "            background-color:  #0c7f43;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col2 {\n",
       "            background-color:  #108647;\n",
       "            color:  #000000;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col1 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col2 {\n",
       "            background-color:  #0c7f43;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col0 {\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col1 {\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col2 {\n",
       "            background-color:  #0c7f43;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >precision</th>        <th class=\"col_heading level0 col1\" >recall</th>        <th class=\"col_heading level0 col2\" >f1score</th>    </tr>    <tr>        <th class=\"index_name level0\" >Unnamed: 0</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row0\" class=\"row_heading level0 row0\" >DecisionTree_Baseline</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col0\" class=\"data row0 col0\" >0.600631</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col1\" class=\"data row0 col1\" >0.0896759</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row0_col2\" class=\"data row0 col2\" >0.156053</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row1\" class=\"row_heading level0 row1\" >RandomForest_Baseline</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col0\" class=\"data row1 col0\" >0.7042</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col1\" class=\"data row1 col1\" >0.0744293</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row1_col2\" class=\"data row1 col2\" >0.134629</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row2\" class=\"row_heading level0 row2\" >GBT_Baseline</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col0\" class=\"data row2 col0\" >0.637856</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col1\" class=\"data row2 col1\" >0.0662527</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row2_col2\" class=\"data row2 col2\" >0.120037</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row3\" class=\"row_heading level0 row3\" >Decision_Tree_Balanced</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col0\" class=\"data row3 col0\" >0.317958</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col1\" class=\"data row3 col1\" >0.606644</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row3_col2\" class=\"data row3 col2\" >0.417233</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row4\" class=\"row_heading level0 row4\" >Random_Forest_Balanced</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col0\" class=\"data row4 col0\" >0.356423</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col1\" class=\"data row4 col1\" >0.576526</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row4_col2\" class=\"data row4 col2\" >0.440511</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row5\" class=\"row_heading level0 row5\" >GBT_Balanced</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col0\" class=\"data row5 col0\" >0.319931</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col1\" class=\"data row5 col1\" >0.626333</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row5_col2\" class=\"data row5 col2\" >0.423526</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row6\" class=\"row_heading level0 row6\" >tree_12bags_12deep</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col0\" class=\"data row6 col0\" >0.322393</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col1\" class=\"data row6 col1\" >0.598598</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row6_col2\" class=\"data row6 col2\" >0.419078</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row7\" class=\"row_heading level0 row7\" >forest_12bags_12deep</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col0\" class=\"data row7 col0\" >0.330183</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col1\" class=\"data row7 col1\" >0.627119</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row7_col2\" class=\"data row7 col2\" >0.4326</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row8\" class=\"row_heading level0 row8\" >gbt_12bags_12deep</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col0\" class=\"data row8 col0\" >0.321464</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col1\" class=\"data row8 col1\" >0.622663</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row8_col2\" class=\"data row8 col2\" >0.424019</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row9\" class=\"row_heading level0 row9\" >LR_Unweighted_DefaultParms</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col0\" class=\"data row9 col0\" >0.559851</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col1\" class=\"data row9 col1\" >0.057988</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row9_col2\" class=\"data row9 col2\" >0.105091</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row10\" class=\"row_heading level0 row10\" >LR_Weighted_DefaultParms</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col0\" class=\"data row10 col0\" >0.318103</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col1\" class=\"data row10 col1\" >0.6247</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row10_col2\" class=\"data row10 col2\" >0.42155</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row11\" class=\"row_heading level0 row11\" >LR_Weighted_OptimalParms</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col0\" class=\"data row11 col0\" >0.311682</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col1\" class=\"data row11 col1\" >0.636287</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row11_col2\" class=\"data row11 col2\" >0.418408</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row12\" class=\"row_heading level0 row12\" >LR_Weighted_OptimalParms_MultiModel</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col0\" class=\"data row12 col0\" >0.311122</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col1\" class=\"data row12 col1\" >0.666963</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row12_col2\" class=\"data row12 col2\" >0.424313</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677level0_row13\" class=\"row_heading level0 row13\" >TestData_Eval_LR_Weighted_OptimalParms_MultiModel</th>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col0\" class=\"data row13 col0\" >0.310467</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col1\" class=\"data row13 col1\" >0.666471</td>\n",
       "                        <td id=\"T_92e4d17e_80ee_11ea_9380_00163e1d8677row13_col2\" class=\"data row13 col2\" >0.423604</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataframe model performance on f1, precision and recall\n",
    "s = metrics_df[['precision','recall','f1score']].style.background_gradient(cmap=\"RdYlGn\")\n",
    "display(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparing model performance on f1, precision and recall\n",
    "\n",
    "fig = plt.figure(figsize=(16,14))\n",
    "#plt.rcParams[\"figure.figsize\"] = (16,14)\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# labels = metrics_df['Unnamed: 0']\n",
    "labels = metrics_df.index\n",
    "plt.plot(labels, metrics_df.precision, color='slateblue', \n",
    "                 linewidth=2, label=\"precision\", marker='o' )\n",
    "plt.plot(labels, metrics_df.recall, color='crimson', \n",
    "                 linewidth=2, label=\"recall\", marker='o' )\n",
    "plt.plot(labels, metrics_df.f1score, color='gray', \n",
    "                 linewidth=2, label=\"f1\", marker='o' )\n",
    "\n",
    "plt.yticks(np.arange(0, .80, .01))\n",
    "plt.ylabel('Performance', fontsize=14)\n",
    "plt.xticks(labels)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Models', fontsize=14, labelpad=20)\n",
    "plt.legend()\n",
    "plt.title(\"Model Performance\")\n",
    "plt.grid(b=True, which='major', color='lightgray', linestyle='-')\n",
    "display(plt.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several course concepts in which our final project helped further expand and embed our understanding. This included concepts around caching, scalability, model complexity, vector embedding, and model assumptions.\n",
    "\n",
    "We learned the importance of caching when attempting to build summary statistics from a dataframe. After producing a 'predictions' dataframe with each row representing a prediction for our dev/test set, we would attempt to use sql queries to count the number of true/false positives and true/false negatives. It was a revelation to us that without caching, the sql queries would produce slightly different numbers from each run, despite our expectation that this would be a static exercise. Caching our predictions dataframe resolvd this issue, and several other similar issues that arose during our efforts to compile results (during bagging, etc). <br>\n",
    "\n",
    "Working with a dataset with 32 million observations and joining to the weather dataset of 600 million observations was a valuable exercise. I/O scaled very well in the Spark Databricks cluster. The speed at which Spark is able to read and write parquet files was astounding.  In addition, our only out-of-memory errors occurred when we switched back to Pandas for some EDA.  We were concerned with possible scaling problems joining to the weather data, but the Spark dataframe API handled left joins from airlines to weather in under 30 minutes clock time with no out-of-memory errors.  The basic MLLib classifiers all scaled well with this dataset, with Logistic Regression typically training in under 2 minutes and Decision trees in under 5 minutes.  Once we elaborated on the models, introducing bagging and boosting, we were at the mercy of cluster load, with some training times exceeding 2 hours.  But on the whole, all of the models scaled very well.\n",
    "\n",
    "Writing the bagging algorithm from scratch really helped us think through and understand the different ways that tree models can be adapted to balance the bias variance tradeoff. While we did not see large gains in our metrics of interest from doing so, implementing this algortithm helped us understand the ways that random forests create high variance trees that are then balanced out through averaging, and that this reduction in variance can be extended by bagging deep trees or deep forests. We thought a lot about model complexity and regularization in the context of the logistic regression models, as well. The gridsearch crossfold validation routines that we ran would sometimes return a \"regParam\" equal to 0, effectively telling us that a model with no regularization was the best choice. On some iterations, though, a small entry would be returned, as well as a \"elasticNetParam\" setting indicating that a regularization blending L1 and L2 was the best implementation.\n",
    "\n",
    "We utilized MLLib's pipeline API for one-hot encoding, standardization, and vector embedding.  In particular, the vector embedding made troubleshooting more challenging as the feature names were obscured.  However; we were able to obtain coefficients for the features in the Logistic Regression model to evaluate the odds ratios for the features and we were able to obtain the feature importance scores from the tree-based models.  Because the airlines dataset has a large number of observations, feature selection did not play a dominant role; however, there are still open questions regarding the appropriateness of one-hot encoding destination and arrival airports given such high cardinality. As such, we expected the tree-based models to perform better since categorical data converted to ordinals is well-tolerated by these models. Indeed, it was a bit of a disappointment to not see much improvement with recall scores, even with bagging and boosting.  \n",
    "\n",
    "Assumptions underlying different algorithms played an important role in our determination of our model choice and subsequently feature engineering efforts. For example, multicollinearity reduces the precision of the estimated weights in our logistic regression model which means we need to place special efforts to remove some features which are highly correlated to other features included in our model. However, a decision tree's ability to utilise highly correlated features effectively and even model interactions between features by the very nature of the structure of trees meant that far less care was required in determining which features to include and exclude.  Due to time limitations, we were not able to fully explore the limits of tree-based models ability to optimize predictions given our set of features - the fact that our ensembled trees utilitisng bagging, random forest and gradient-boosted trees did not result in notably improved performance over our standard decision tree works against our intuition and brings suspect to our feature engineering efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etani, Noriko (2019). \"Development of a predictive model for ontime arrival flight of airliner by discovering correlation between flight and weather data.\" Journal of Big Data, 6:85.\n",
    "\n",
    "James, Witten, Hastie, and Tibshirani (2017). \"An Introduction to Statistical Learning,\" Springer, chapters 4, 5, 6, and 8.\n",
    "\n",
    "Kafle, Nabin, and Bo Zou (2016). Modeling Flight Delay Propagation: A New Analytical-Econometric Approach. Transportation Research Part B: Methodological, Pergamon, 1 Sept. 2016, www.sciencedirect.com/science/article/pii/S0191261515302010.\n",
    "\n",
    "Parr, Terence and Howard, Jeremy (2020). \"How to explain gradient boosting.\" Accessed in 2020: https://explained.ai/gradient-boosting/index.html.\n",
    "\n",
    "Patgiri, Ripon, Sajid Hussain and Aditya Nongmeikapam (2020). \"Empirical Study on Airline Delay Analysis and Prediction.\"\" arXiv. Accessed at: https://arxiv.org/abs/2002.10254\n",
    "\n",
    "Thiagarajan, B., L. Srinivasan, A. V. Sharma, D. Sreekanthan and V. Vijayaraghavan (2017).\"A machine learning approach for prediction of on-time performance of flights,\" IEEE/AIAA 36th Digital Avionics Systems Conference (DASC), St. Petersburg, FL, 2017, pp. 1-6.  \n",
    "\n",
    "The following document contains additional references to our research/literature review  \n",
    "https://docs.google.com/document/d/1AzO84jtXgSrKxz9oSo1jpRQUxAApW1fJcQDHvYE1Jgw/edit?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "W261_SP20_FINAL_PROJECT_TEAM3",
  "notebookId": 1239491467262944
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
